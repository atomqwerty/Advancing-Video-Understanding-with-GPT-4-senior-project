{"text": " Hello, welcome to the Course Erich course on your all networks for machine learning. Before we get into the details of neural network learning algorithms, I want to talk a little bit about machine learning. Why we need machine learning? The kinds of things we use it for and show you some examples of what it can do. The reason we need machine learning is that the some problems where it's very hard to write the programs. Recognizing a three-dimensional object, for example, when it's from a novel viewpoint in new lighting conditions in a cluttered scene, is very hard to do. We don't know what programs are right because we don't know how it's done in our brain. And even if we did know what programs are right, it might be that it was a horrendously complicated program. Another example is detecting a fraudulent credit card transaction, where there may not be any nice simple rules that will tell you it's fraudulent. You really need to combine a very large number of not very reliable rules. And also those rules change over time because people change the tricks they use for fraud. So we need a complicated program that combines unreliable rules and that we can change easily. The machine learning approach is to say instead of writing each program by hand for each specific task, for a particular task we're collecting a lot of examples that specify the correct output for a given input. A machine learning algorithm then takes these examples and produces a program that does the job. The program produced by the learning algorithm may look very different from a typical handwritten program. For example it might contain millions of numbers about how you wait different kinds of evidence. If we do it right the program should work for new cases as well as the ones it's trained on. And if the data changes, we should be able to change the program relatively easily by retrain it on the new data. And now, mass romance or computation, a cheaper than paying someone to write a program for a specific task. So we can afford big complicated machine learning programs to produce the task specific systems for us. Some examples of the things that are best done by using a learning algorithm are recognizing patterns. So for example objects in real scenes or the identities or expressions of people's faces or spoken words. There's also recognizing anomalies. So an unusual sequence of credit card transactions would be anonymly. Another example of anonymly would be an unusual pattern of sensor readings in a nuclear power plant. And you wouldn't really want to have to deal with those by doing supervised learning where you look at the ones that blow up and see what, what cause them to blow up. You'd really like to recognize that something funny is happening without having any supervision signal. It's just not behaving in its normal way. And then this prediction, so typically predicting future stock prices or currency exchange rates, or predicting which movies are person were like, from knowing which other movies they like and which movies a lot of other people like. So in this course, I'm going to use a standard example for explaining a lot of the machine learning algorithms. This is done in a lot of science. In genetics for example, a lot of genetics is done on fruit flies. And the reason is their convenient. They breed fast and a lot is already known about the genesis of fruit flies. The emnis database of handwritten digits is the machine learning equivalent of fruit flies. It's publicly available. We can get machine learning algorithms to learn how to recognize these handwritten digits quite quickly. So it's easy to try lots of variations. And we know huge amounts about how well different machine learning methods do on emnist. And in particular, the different machine learning methods were implemented by people who believed in them, so we can rely on those results. So for all those reasons, we're going to use emnist as our standard task. Here's an example of some of the digits in emnist. These are ones that were correctly recognised by neural net, the first time it saw them. But they're ones where the neural net wasn't very confident. And you can see why. I've arranged these digits in standard scanline order, so zero is then one's end to is and so on. If you look at a bunch of twos, like the ones in the green rectangle, you can see that. If you knew they were a handwritten digit, you'd probably guess they were twos. But it's very hard to say what it is that makes them twos. There's nothing simple that they all have in common. In particular, if you try an overlay one on another, you'll see it doesn't fit. And even if you skew it a bit, it's very hard to make them overlay on each other, so template isn't going to do the job. And in particular, template is going to be very hard to find that it'll fit those twos in the green box and won't also fit the things in the red boxes. So that's one thing that makes recognising handwritten digits, a good task for machine learning. Now, I don't want you to think that's the only thing we can do. It's a relatively simple thing for a machine learning system to do now. And to motivate the rest of the course, I want to show you some examples of much more difficult things. So we now have neural nets with approaching 100 million parameters in them. And that can recognize a thousand different object classes in 1.3 million high resolution training images got from the web. So there was a competition in 2010 and the best system got 47% error rate if you look at its first choice. And 25% error rate, if you say it got it right if it was in its top five choices, which isn't bad for a thousand different objects. Jitendromalic, who's an eminent neural net skeptic and a leading computer vision researcher, has said that this competition is a good test of whether deep neural networks can work well for object recognition. And a very deep neural network can now do considerably better than the thing that won the competition. It can get less than 40% error for its first choice and less than 20% error for its top five choices. I'll describe that in much more detail in lecture five. Here's some examples of the kinds of images you have to recognize. These images from the test set that is never seen before. And below the examples, I'm showing you what the neural net thought the right answer was, where the length of the horizontal bias, how confident it was, and the correct answer is in red. So if you look in the middle, it correctly identified that as a snow plow. But you can see that it's other choices, we're also fairly sensible. It does look a little bit like a drilling platform. And if you look at its third choice, a lifeboat, it actually looks very like a lifeboat. You can see the flag on the front of the boat and the bridge of the boat and the flag at the back and the high surf in the background. So it's, it's errors tell you a lot about how it's doing it and they're very plausible errors. If you look on the left, it gets it wrong, possibly because the beak of the bird is missing and because the feathers of the bird look very like the wet fur of an auto. But it gets in in this top five and it does better than me. I wouldn't know if that was a quail or a roughed grass or a partridge. If you look on the right, it gets a completely wrong. It, um, agility, you can see why it says that. You can possibly see why it says orangutan because of the sort of jungle looking background is something orange in the middle. But it fails to get the right answer. It can, however, deal with a wide range of different objects. If you look on the left, I would have said micro-overs, my first answer. The labels are very systematic. So actually the correct answer there is electric range and does get it in this top five. In the middle it's getting a turn star which is distributed object. It can do more than just recognise compact things. And it can also deal with pictures as well as real scenes like the bulletproof vest. And it makes some very cool errors. If you look at the image on the left, that's an earphone. It doesn't get anything like an earphone, but if you look at it as fourth bet, it thinks it's an ant. And to be with you, think that's crazy. But then if you look at it carefully, you can see it's a view from an ant from underneath. The eyes are looking down at you. And you can see the antennae behind it. It's not the kind of view of an ant you'd like to have if you were a green fly. If you look at the one on the right, it doesn't get the right answer, but all of his answers are cylindrical objects. Another task that you're on that's an I very good at is speech recognition. Or at least part of a speech recognition system. So speech recognition systems have several stages. First they pre-process the sound wave to get a vector of acoustic coefficients for each 10 milliseconds of sound wave. And so they get 100 of those vectors per second. They then take a few adjacent vectors of acoustic coefficients. And they need to place bets on which part of which phoneme is being spoken. So they look at this little window and they say in the middle of this window, what do I think the phoneme is and which part of the phoneme is it? And a good speech recognition system will have many alternative models for a phoneme. And each model it might have three different parts. So it might have many thousands of alternative fragments that it thinks this might be. And you have to place bets on all those thousands of alternatives. And then once you place those bets, you have a decoding stage that does the best job it can of using plausible bets, but piecing them together into a sequence of bets that corresponds to the kinds of things that people say. Currently deep neural networks pioneered by George Darwin, Abdul Rahman, Muhammad at the University of Toronto, are doing better than previous machine learning methods for the acoustic model. And then I begin to be used in practical systems. So, Dalam Muhammad developed a system that uses many layers of binary neurons. To take some acoustic frames and make bets about the labels. They were doing it on a fairly small database and they are used 183 alternative labels. And to get this system to well, they did some pre-training which will be described in the second half of the course. After standard post-processing, they got 20.7% error rate on a very standard benchmark, which is kind of like the M-nist for speech. The best previous result on that benchmark for speaker independent recognition was 24.4%. And the very experience for speech researcher at Microsoft Research realized that that was a big enough improvement that probably this would change the way speech recognition systems were done. And indeed it has. So, if you look at recent results from several different leading speech groups, Microsoft showed that this kind of deep neural network, when used as the acoustic model in a speech system, reduced the error rate from 37.4% to 18.5%. Or alternatively, you could view it as reducing the amount of training data you needed from 2000 hours down to 3009 hours to get comparable performance. IBM, which has the best system for one of the standard speech recognition tasks for large recovery speech recognition, showed that even its very highly tuned system that was getting 18.8% can be beaten by one of these deep neural networks. And Google, fairly recently, trained deep neural network on a large amount of speech, 5,800 hours. That was still much less than their trained, they got to in mixumod along. But even with much less data, it did a lot better than the technology they had before. So, it reduced the error rate from 16% to 12.3% and the error rate is still falling. And in the latest Android, if you do voice search, it's using one of these deep neural networks in order to do very good speech recognition.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " Hello, welcome to the Course Erich course on your all networks for machine learning.", "tokens": [50364, 2425, 11, 2928, 281, 264, 27327, 3300, 480, 1164, 322, 428, 439, 9590, 337, 3479, 2539, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2923728733846586, "compression_ratio": 1.6205128205128205, "no_speech_prob": 0.08952748030424118}, {"id": 1, "seek": 0, "start": 7.0, "end": 14.0, "text": " Before we get into the details of neural network learning algorithms, I want to talk a little bit about machine learning.", "tokens": [50714, 4546, 321, 483, 666, 264, 4365, 295, 18161, 3209, 2539, 14642, 11, 286, 528, 281, 751, 257, 707, 857, 466, 3479, 2539, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2923728733846586, "compression_ratio": 1.6205128205128205, "no_speech_prob": 0.08952748030424118}, {"id": 2, "seek": 0, "start": 14.0, "end": 22.0, "text": " Why we need machine learning? The kinds of things we use it for and show you some examples of what it can do.", "tokens": [51064, 1545, 321, 643, 3479, 2539, 30, 440, 3685, 295, 721, 321, 764, 309, 337, 293, 855, 291, 512, 5110, 295, 437, 309, 393, 360, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2923728733846586, "compression_ratio": 1.6205128205128205, "no_speech_prob": 0.08952748030424118}, {"id": 3, "seek": 2200, "start": 22.0, "end": 29.0, "text": " The reason we need machine learning is that the some problems where it's very hard to write the programs.", "tokens": [50364, 440, 1778, 321, 643, 3479, 2539, 307, 300, 264, 512, 2740, 689, 309, 311, 588, 1152, 281, 2464, 264, 4268, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1468341372428684, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.10485253483057022}, {"id": 4, "seek": 2200, "start": 29.0, "end": 38.0, "text": " Recognizing a three-dimensional object, for example, when it's from a novel viewpoint in new lighting conditions in a cluttered scene, is very hard to do.", "tokens": [50714, 44682, 3319, 257, 1045, 12, 18759, 2657, 11, 337, 1365, 11, 562, 309, 311, 490, 257, 7613, 35248, 294, 777, 9577, 4487, 294, 257, 40614, 292, 4145, 11, 307, 588, 1152, 281, 360, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1468341372428684, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.10485253483057022}, {"id": 5, "seek": 2200, "start": 38.0, "end": 43.0, "text": " We don't know what programs are right because we don't know how it's done in our brain.", "tokens": [51164, 492, 500, 380, 458, 437, 4268, 366, 558, 570, 321, 500, 380, 458, 577, 309, 311, 1096, 294, 527, 3567, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1468341372428684, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.10485253483057022}, {"id": 6, "seek": 2200, "start": 43.0, "end": 50.0, "text": " And even if we did know what programs are right, it might be that it was a horrendously complicated program.", "tokens": [51414, 400, 754, 498, 321, 630, 458, 437, 4268, 366, 558, 11, 309, 1062, 312, 300, 309, 390, 257, 49520, 5098, 6179, 1461, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1468341372428684, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.10485253483057022}, {"id": 7, "seek": 5000, "start": 51.0, "end": 60.0, "text": " Another example is detecting a fraudulent credit card transaction, where there may not be any nice simple rules that will tell you it's fraudulent.", "tokens": [50414, 3996, 1365, 307, 40237, 257, 14560, 23405, 5397, 2920, 14425, 11, 689, 456, 815, 406, 312, 604, 1481, 2199, 4474, 300, 486, 980, 291, 309, 311, 14560, 23405, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10231093300713433, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0675223246216774}, {"id": 8, "seek": 5000, "start": 60.0, "end": 65.0, "text": " You really need to combine a very large number of not very reliable rules.", "tokens": [50864, 509, 534, 643, 281, 10432, 257, 588, 2416, 1230, 295, 406, 588, 12924, 4474, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10231093300713433, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0675223246216774}, {"id": 9, "seek": 5000, "start": 65.0, "end": 71.0, "text": " And also those rules change over time because people change the tricks they use for fraud.", "tokens": [51114, 400, 611, 729, 4474, 1319, 670, 565, 570, 561, 1319, 264, 11733, 436, 764, 337, 14560, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10231093300713433, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0675223246216774}, {"id": 10, "seek": 5000, "start": 71.0, "end": 78.0, "text": " So we need a complicated program that combines unreliable rules and that we can change easily.", "tokens": [51414, 407, 321, 643, 257, 6179, 1461, 300, 29520, 20584, 2081, 712, 4474, 293, 300, 321, 393, 1319, 3612, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10231093300713433, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0675223246216774}, {"id": 11, "seek": 7800, "start": 79.0, "end": 93.0, "text": " The machine learning approach is to say instead of writing each program by hand for each specific task, for a particular task we're collecting a lot of examples that specify the correct output for a given input.", "tokens": [50414, 440, 3479, 2539, 3109, 307, 281, 584, 2602, 295, 3579, 1184, 1461, 538, 1011, 337, 1184, 2685, 5633, 11, 337, 257, 1729, 5633, 321, 434, 12510, 257, 688, 295, 5110, 300, 16500, 264, 3006, 5598, 337, 257, 2212, 4846, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12135936553219714, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.0425337515771389}, {"id": 12, "seek": 7800, "start": 93.0, "end": 99.0, "text": " A machine learning algorithm then takes these examples and produces a program that does the job.", "tokens": [51114, 316, 3479, 2539, 9284, 550, 2516, 613, 5110, 293, 14725, 257, 1461, 300, 775, 264, 1691, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12135936553219714, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.0425337515771389}, {"id": 13, "seek": 7800, "start": 99.0, "end": 106.0, "text": " The program produced by the learning algorithm may look very different from a typical handwritten program.", "tokens": [51414, 440, 1461, 7126, 538, 264, 2539, 9284, 815, 574, 588, 819, 490, 257, 7476, 1011, 26859, 1461, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12135936553219714, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.0425337515771389}, {"id": 14, "seek": 10600, "start": 106.0, "end": 112.0, "text": " For example it might contain millions of numbers about how you wait different kinds of evidence.", "tokens": [50364, 1171, 1365, 309, 1062, 5304, 6803, 295, 3547, 466, 577, 291, 1699, 819, 3685, 295, 4467, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11644876003265381, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.02551104687154293}, {"id": 15, "seek": 10600, "start": 112.0, "end": 118.0, "text": " If we do it right the program should work for new cases as well as the ones it's trained on.", "tokens": [50664, 759, 321, 360, 309, 558, 264, 1461, 820, 589, 337, 777, 3331, 382, 731, 382, 264, 2306, 309, 311, 8895, 322, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11644876003265381, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.02551104687154293}, {"id": 16, "seek": 10600, "start": 118.0, "end": 127.0, "text": " And if the data changes, we should be able to change the program relatively easily by retrain it on the new data.", "tokens": [50964, 400, 498, 264, 1412, 2962, 11, 321, 820, 312, 1075, 281, 1319, 264, 1461, 7226, 3612, 538, 1533, 7146, 309, 322, 264, 777, 1412, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11644876003265381, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.02551104687154293}, {"id": 17, "seek": 12700, "start": 127.0, "end": 133.0, "text": " And now, mass romance or computation, a cheaper than paying someone to write a program for a specific task.", "tokens": [50364, 400, 586, 11, 2758, 19064, 420, 24903, 11, 257, 12284, 813, 6229, 1580, 281, 2464, 257, 1461, 337, 257, 2685, 5633, 13, 50664], "temperature": 0.0, "avg_logprob": -0.19766836455374054, "compression_ratio": 1.5939086294416243, "no_speech_prob": 0.03320696949958801}, {"id": 18, "seek": 12700, "start": 133.0, "end": 142.0, "text": " So we can afford big complicated machine learning programs to produce the task specific systems for us.", "tokens": [50664, 407, 321, 393, 6157, 955, 6179, 3479, 2539, 4268, 281, 5258, 264, 5633, 2685, 3652, 337, 505, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19766836455374054, "compression_ratio": 1.5939086294416243, "no_speech_prob": 0.03320696949958801}, {"id": 19, "seek": 12700, "start": 142.0, "end": 150.0, "text": " Some examples of the things that are best done by using a learning algorithm are recognizing patterns.", "tokens": [51114, 2188, 5110, 295, 264, 721, 300, 366, 1151, 1096, 538, 1228, 257, 2539, 9284, 366, 18538, 8294, 13, 51514], "temperature": 0.0, "avg_logprob": -0.19766836455374054, "compression_ratio": 1.5939086294416243, "no_speech_prob": 0.03320696949958801}, {"id": 20, "seek": 15000, "start": 150.0, "end": 160.0, "text": " So for example objects in real scenes or the identities or expressions of people's faces or spoken words.", "tokens": [50364, 407, 337, 1365, 6565, 294, 957, 8026, 420, 264, 24239, 420, 15277, 295, 561, 311, 8475, 420, 10759, 2283, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1806463296862616, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.1756821572780609}, {"id": 21, "seek": 15000, "start": 160.0, "end": 164.0, "text": " There's also recognizing anomalies.", "tokens": [50864, 821, 311, 611, 18538, 24769, 48872, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1806463296862616, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.1756821572780609}, {"id": 22, "seek": 15000, "start": 164.0, "end": 169.0, "text": " So an unusual sequence of credit card transactions would be anonymly.", "tokens": [51064, 407, 364, 10901, 8310, 295, 5397, 2920, 16856, 576, 312, 37293, 356, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1806463296862616, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.1756821572780609}, {"id": 23, "seek": 15000, "start": 169.0, "end": 175.0, "text": " Another example of anonymly would be an unusual pattern of sensor readings in a nuclear power plant.", "tokens": [51314, 3996, 1365, 295, 37293, 356, 576, 312, 364, 10901, 5102, 295, 10200, 27319, 294, 257, 8179, 1347, 3709, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1806463296862616, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.1756821572780609}, {"id": 24, "seek": 17500, "start": 175.0, "end": 184.0, "text": " And you wouldn't really want to have to deal with those by doing supervised learning where you look at the ones that blow up and see what, what cause them to blow up.", "tokens": [50364, 400, 291, 2759, 380, 534, 528, 281, 362, 281, 2028, 365, 729, 538, 884, 46533, 2539, 689, 291, 574, 412, 264, 2306, 300, 6327, 493, 293, 536, 437, 11, 437, 3082, 552, 281, 6327, 493, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14421442576817103, "compression_ratio": 1.6630434782608696, "no_speech_prob": 0.20326973497867584}, {"id": 25, "seek": 17500, "start": 184.0, "end": 189.0, "text": " You'd really like to recognize that something funny is happening without having any supervision signal.", "tokens": [50814, 509, 1116, 534, 411, 281, 5521, 300, 746, 4074, 307, 2737, 1553, 1419, 604, 32675, 6358, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14421442576817103, "compression_ratio": 1.6630434782608696, "no_speech_prob": 0.20326973497867584}, {"id": 26, "seek": 17500, "start": 189.0, "end": 192.0, "text": " It's just not behaving in its normal way.", "tokens": [51064, 467, 311, 445, 406, 35263, 294, 1080, 2710, 636, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14421442576817103, "compression_ratio": 1.6630434782608696, "no_speech_prob": 0.20326973497867584}, {"id": 27, "seek": 17500, "start": 192.0, "end": 202.0, "text": " And then this prediction, so typically predicting future stock prices or currency exchange rates, or predicting which movies are person were like,", "tokens": [51214, 400, 550, 341, 17630, 11, 370, 5850, 32884, 2027, 4127, 7901, 420, 13346, 7742, 6846, 11, 420, 32884, 597, 6233, 366, 954, 645, 411, 11, 51714], "temperature": 0.0, "avg_logprob": -0.14421442576817103, "compression_ratio": 1.6630434782608696, "no_speech_prob": 0.20326973497867584}, {"id": 28, "seek": 20200, "start": 202.0, "end": 208.0, "text": " from knowing which other movies they like and which movies a lot of other people like.", "tokens": [50364, 490, 5276, 597, 661, 6233, 436, 411, 293, 597, 6233, 257, 688, 295, 661, 561, 411, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13913623491923013, "compression_ratio": 1.6797752808988764, "no_speech_prob": 0.015529025346040726}, {"id": 29, "seek": 20200, "start": 208.0, "end": 217.0, "text": " So in this course, I'm going to use a standard example for explaining a lot of the machine learning algorithms.", "tokens": [50664, 407, 294, 341, 1164, 11, 286, 478, 516, 281, 764, 257, 3832, 1365, 337, 13468, 257, 688, 295, 264, 3479, 2539, 14642, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13913623491923013, "compression_ratio": 1.6797752808988764, "no_speech_prob": 0.015529025346040726}, {"id": 30, "seek": 20200, "start": 217.0, "end": 223.0, "text": " This is done in a lot of science. In genetics for example, a lot of genetics is done on fruit flies.", "tokens": [51114, 639, 307, 1096, 294, 257, 688, 295, 3497, 13, 682, 26516, 337, 1365, 11, 257, 688, 295, 26516, 307, 1096, 322, 6773, 17414, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13913623491923013, "compression_ratio": 1.6797752808988764, "no_speech_prob": 0.015529025346040726}, {"id": 31, "seek": 22300, "start": 224.0, "end": 232.0, "text": " And the reason is their convenient. They breed fast and a lot is already known about the genesis of fruit flies.", "tokens": [50414, 400, 264, 1778, 307, 641, 10851, 13, 814, 18971, 2370, 293, 257, 688, 307, 1217, 2570, 466, 264, 1049, 9374, 295, 6773, 17414, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18375771621177936, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.08024036139249802}, {"id": 32, "seek": 22300, "start": 232.0, "end": 240.0, "text": " The emnis database of handwritten digits is the machine learning equivalent of fruit flies.", "tokens": [50814, 440, 846, 77, 271, 8149, 295, 1011, 26859, 27011, 307, 264, 3479, 2539, 10344, 295, 6773, 17414, 13, 51214], "temperature": 0.0, "avg_logprob": -0.18375771621177936, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.08024036139249802}, {"id": 33, "seek": 22300, "start": 240.0, "end": 242.0, "text": " It's publicly available.", "tokens": [51214, 467, 311, 14843, 2435, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18375771621177936, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.08024036139249802}, {"id": 34, "seek": 22300, "start": 242.0, "end": 249.0, "text": " We can get machine learning algorithms to learn how to recognize these handwritten digits quite quickly.", "tokens": [51314, 492, 393, 483, 3479, 2539, 14642, 281, 1466, 577, 281, 5521, 613, 1011, 26859, 27011, 1596, 2661, 13, 51664], "temperature": 0.0, "avg_logprob": -0.18375771621177936, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.08024036139249802}, {"id": 35, "seek": 22300, "start": 249.0, "end": 252.0, "text": " So it's easy to try lots of variations.", "tokens": [51664, 407, 309, 311, 1858, 281, 853, 3195, 295, 17840, 13, 51814], "temperature": 0.0, "avg_logprob": -0.18375771621177936, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.08024036139249802}, {"id": 36, "seek": 25200, "start": 252.0, "end": 257.0, "text": " And we know huge amounts about how well different machine learning methods do on emnist.", "tokens": [50364, 400, 321, 458, 2603, 11663, 466, 577, 731, 819, 3479, 2539, 7150, 360, 322, 846, 77, 468, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08810814295973733, "compression_ratio": 1.7131782945736433, "no_speech_prob": 0.01624923199415207}, {"id": 37, "seek": 25200, "start": 257.0, "end": 265.0, "text": " And in particular, the different machine learning methods were implemented by people who believed in them, so we can rely on those results.", "tokens": [50614, 400, 294, 1729, 11, 264, 819, 3479, 2539, 7150, 645, 12270, 538, 561, 567, 7847, 294, 552, 11, 370, 321, 393, 10687, 322, 729, 3542, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08810814295973733, "compression_ratio": 1.7131782945736433, "no_speech_prob": 0.01624923199415207}, {"id": 38, "seek": 25200, "start": 265.0, "end": 270.0, "text": " So for all those reasons, we're going to use emnist as our standard task.", "tokens": [51014, 407, 337, 439, 729, 4112, 11, 321, 434, 516, 281, 764, 846, 77, 468, 382, 527, 3832, 5633, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08810814295973733, "compression_ratio": 1.7131782945736433, "no_speech_prob": 0.01624923199415207}, {"id": 39, "seek": 25200, "start": 270.0, "end": 274.0, "text": " Here's an example of some of the digits in emnist.", "tokens": [51264, 1692, 311, 364, 1365, 295, 512, 295, 264, 27011, 294, 846, 77, 468, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08810814295973733, "compression_ratio": 1.7131782945736433, "no_speech_prob": 0.01624923199415207}, {"id": 40, "seek": 25200, "start": 274.0, "end": 280.0, "text": " These are ones that were correctly recognised by neural net, the first time it saw them.", "tokens": [51464, 1981, 366, 2306, 300, 645, 8944, 36802, 538, 18161, 2533, 11, 264, 700, 565, 309, 1866, 552, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08810814295973733, "compression_ratio": 1.7131782945736433, "no_speech_prob": 0.01624923199415207}, {"id": 41, "seek": 28000, "start": 281.0, "end": 284.0, "text": " But they're ones where the neural net wasn't very confident.", "tokens": [50414, 583, 436, 434, 2306, 689, 264, 18161, 2533, 2067, 380, 588, 6679, 13, 50564], "temperature": 0.0, "avg_logprob": -0.22105327094953084, "compression_ratio": 1.6214953271028036, "no_speech_prob": 0.06920050829648972}, {"id": 42, "seek": 28000, "start": 284.0, "end": 286.0, "text": " And you can see why.", "tokens": [50564, 400, 291, 393, 536, 983, 13, 50664], "temperature": 0.0, "avg_logprob": -0.22105327094953084, "compression_ratio": 1.6214953271028036, "no_speech_prob": 0.06920050829648972}, {"id": 43, "seek": 28000, "start": 286.0, "end": 294.0, "text": " I've arranged these digits in standard scanline order, so zero is then one's end to is and so on.", "tokens": [50664, 286, 600, 18721, 613, 27011, 294, 3832, 11049, 1889, 1668, 11, 370, 4018, 307, 550, 472, 311, 917, 281, 307, 293, 370, 322, 13, 51064], "temperature": 0.0, "avg_logprob": -0.22105327094953084, "compression_ratio": 1.6214953271028036, "no_speech_prob": 0.06920050829648972}, {"id": 44, "seek": 28000, "start": 294.0, "end": 300.0, "text": " If you look at a bunch of twos, like the ones in the green rectangle, you can see that.", "tokens": [51064, 759, 291, 574, 412, 257, 3840, 295, 683, 329, 11, 411, 264, 2306, 294, 264, 3092, 21930, 11, 291, 393, 536, 300, 13, 51364], "temperature": 0.0, "avg_logprob": -0.22105327094953084, "compression_ratio": 1.6214953271028036, "no_speech_prob": 0.06920050829648972}, {"id": 45, "seek": 28000, "start": 300.0, "end": 305.0, "text": " If you knew they were a handwritten digit, you'd probably guess they were twos.", "tokens": [51364, 759, 291, 2586, 436, 645, 257, 1011, 26859, 14293, 11, 291, 1116, 1391, 2041, 436, 645, 683, 329, 13, 51614], "temperature": 0.0, "avg_logprob": -0.22105327094953084, "compression_ratio": 1.6214953271028036, "no_speech_prob": 0.06920050829648972}, {"id": 46, "seek": 30500, "start": 306.0, "end": 309.0, "text": " But it's very hard to say what it is that makes them twos.", "tokens": [50414, 583, 309, 311, 588, 1152, 281, 584, 437, 309, 307, 300, 1669, 552, 683, 329, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10436597959263118, "compression_ratio": 1.848605577689243, "no_speech_prob": 0.12694765627384186}, {"id": 47, "seek": 30500, "start": 309.0, "end": 312.0, "text": " There's nothing simple that they all have in common.", "tokens": [50564, 821, 311, 1825, 2199, 300, 436, 439, 362, 294, 2689, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10436597959263118, "compression_ratio": 1.848605577689243, "no_speech_prob": 0.12694765627384186}, {"id": 48, "seek": 30500, "start": 312.0, "end": 317.0, "text": " In particular, if you try an overlay one on another, you'll see it doesn't fit.", "tokens": [50714, 682, 1729, 11, 498, 291, 853, 364, 31741, 472, 322, 1071, 11, 291, 603, 536, 309, 1177, 380, 3318, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10436597959263118, "compression_ratio": 1.848605577689243, "no_speech_prob": 0.12694765627384186}, {"id": 49, "seek": 30500, "start": 317.0, "end": 323.0, "text": " And even if you skew it a bit, it's very hard to make them overlay on each other, so template isn't going to do the job.", "tokens": [50964, 400, 754, 498, 291, 8756, 86, 309, 257, 857, 11, 309, 311, 588, 1152, 281, 652, 552, 31741, 322, 1184, 661, 11, 370, 12379, 1943, 380, 516, 281, 360, 264, 1691, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10436597959263118, "compression_ratio": 1.848605577689243, "no_speech_prob": 0.12694765627384186}, {"id": 50, "seek": 30500, "start": 323.0, "end": 333.0, "text": " And in particular, template is going to be very hard to find that it'll fit those twos in the green box and won't also fit the things in the red boxes.", "tokens": [51264, 400, 294, 1729, 11, 12379, 307, 516, 281, 312, 588, 1152, 281, 915, 300, 309, 603, 3318, 729, 683, 329, 294, 264, 3092, 2424, 293, 1582, 380, 611, 3318, 264, 721, 294, 264, 2182, 9002, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10436597959263118, "compression_ratio": 1.848605577689243, "no_speech_prob": 0.12694765627384186}, {"id": 51, "seek": 33300, "start": 333.0, "end": 339.0, "text": " So that's one thing that makes recognising handwritten digits, a good task for machine learning.", "tokens": [50364, 407, 300, 311, 472, 551, 300, 1669, 3068, 3436, 1011, 26859, 27011, 11, 257, 665, 5633, 337, 3479, 2539, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09908001965815479, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.024728402495384216}, {"id": 52, "seek": 33300, "start": 339.0, "end": 343.0, "text": " Now, I don't want you to think that's the only thing we can do.", "tokens": [50664, 823, 11, 286, 500, 380, 528, 291, 281, 519, 300, 311, 264, 787, 551, 321, 393, 360, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09908001965815479, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.024728402495384216}, {"id": 53, "seek": 33300, "start": 343.0, "end": 347.0, "text": " It's a relatively simple thing for a machine learning system to do now.", "tokens": [50864, 467, 311, 257, 7226, 2199, 551, 337, 257, 3479, 2539, 1185, 281, 360, 586, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09908001965815479, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.024728402495384216}, {"id": 54, "seek": 33300, "start": 347.0, "end": 353.0, "text": " And to motivate the rest of the course, I want to show you some examples of much more difficult things.", "tokens": [51064, 400, 281, 28497, 264, 1472, 295, 264, 1164, 11, 286, 528, 281, 855, 291, 512, 5110, 295, 709, 544, 2252, 721, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09908001965815479, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.024728402495384216}, {"id": 55, "seek": 33300, "start": 353.0, "end": 359.0, "text": " So we now have neural nets with approaching 100 million parameters in them.", "tokens": [51364, 407, 321, 586, 362, 18161, 36170, 365, 14908, 2319, 2459, 9834, 294, 552, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09908001965815479, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.024728402495384216}, {"id": 56, "seek": 35900, "start": 359.0, "end": 369.0, "text": " And that can recognize a thousand different object classes in 1.3 million high resolution training images got from the web.", "tokens": [50364, 400, 300, 393, 5521, 257, 4714, 819, 2657, 5359, 294, 502, 13, 18, 2459, 1090, 8669, 3097, 5267, 658, 490, 264, 3670, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13360061864743286, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.06543246656656265}, {"id": 57, "seek": 35900, "start": 369.0, "end": 377.0, "text": " So there was a competition in 2010 and the best system got 47% error rate if you look at its first choice.", "tokens": [50864, 407, 456, 390, 257, 6211, 294, 9657, 293, 264, 1151, 1185, 658, 16953, 4, 6713, 3314, 498, 291, 574, 412, 1080, 700, 3922, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13360061864743286, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.06543246656656265}, {"id": 58, "seek": 35900, "start": 377.0, "end": 385.0, "text": " And 25% error rate, if you say it got it right if it was in its top five choices, which isn't bad for a thousand different objects.", "tokens": [51264, 400, 3552, 4, 6713, 3314, 11, 498, 291, 584, 309, 658, 309, 558, 498, 309, 390, 294, 1080, 1192, 1732, 7994, 11, 597, 1943, 380, 1578, 337, 257, 4714, 819, 6565, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13360061864743286, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.06543246656656265}, {"id": 59, "seek": 38500, "start": 385.0, "end": 399.0, "text": " Jitendromalic, who's an eminent neural net skeptic and a leading computer vision researcher, has said that this competition is a good test of whether deep neural networks can work well for object recognition.", "tokens": [50364, 508, 270, 521, 4397, 304, 299, 11, 567, 311, 364, 846, 11058, 18161, 2533, 19128, 299, 293, 257, 5775, 3820, 5201, 21751, 11, 575, 848, 300, 341, 6211, 307, 257, 665, 1500, 295, 1968, 2452, 18161, 9590, 393, 589, 731, 337, 2657, 11150, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15263435966090153, "compression_ratio": 1.7208333333333334, "no_speech_prob": 0.05928707495331764}, {"id": 60, "seek": 38500, "start": 399.0, "end": 407.0, "text": " And a very deep neural network can now do considerably better than the thing that won the competition.", "tokens": [51064, 400, 257, 588, 2452, 18161, 3209, 393, 586, 360, 31308, 1101, 813, 264, 551, 300, 1582, 264, 6211, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15263435966090153, "compression_ratio": 1.7208333333333334, "no_speech_prob": 0.05928707495331764}, {"id": 61, "seek": 38500, "start": 407.0, "end": 413.0, "text": " It can get less than 40% error for its first choice and less than 20% error for its top five choices.", "tokens": [51464, 467, 393, 483, 1570, 813, 3356, 4, 6713, 337, 1080, 700, 3922, 293, 1570, 813, 945, 4, 6713, 337, 1080, 1192, 1732, 7994, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15263435966090153, "compression_ratio": 1.7208333333333334, "no_speech_prob": 0.05928707495331764}, {"id": 62, "seek": 41300, "start": 413.0, "end": 419.0, "text": " I'll describe that in much more detail in lecture five. Here's some examples of the kinds of images you have to recognize.", "tokens": [50364, 286, 603, 6786, 300, 294, 709, 544, 2607, 294, 7991, 1732, 13, 1692, 311, 512, 5110, 295, 264, 3685, 295, 5267, 291, 362, 281, 5521, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13398518673209256, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.06052589416503906}, {"id": 63, "seek": 41300, "start": 419.0, "end": 423.0, "text": " These images from the test set that is never seen before.", "tokens": [50664, 1981, 5267, 490, 264, 1500, 992, 300, 307, 1128, 1612, 949, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13398518673209256, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.06052589416503906}, {"id": 64, "seek": 41300, "start": 423.0, "end": 436.0, "text": " And below the examples, I'm showing you what the neural net thought the right answer was, where the length of the horizontal bias, how confident it was, and the correct answer is in red.", "tokens": [50864, 400, 2507, 264, 5110, 11, 286, 478, 4099, 291, 437, 264, 18161, 2533, 1194, 264, 558, 1867, 390, 11, 689, 264, 4641, 295, 264, 12750, 12577, 11, 577, 6679, 309, 390, 11, 293, 264, 3006, 1867, 307, 294, 2182, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13398518673209256, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.06052589416503906}, {"id": 65, "seek": 43600, "start": 436.0, "end": 440.0, "text": " So if you look in the middle, it correctly identified that as a snow plow.", "tokens": [50364, 407, 498, 291, 574, 294, 264, 2808, 11, 309, 8944, 9234, 300, 382, 257, 5756, 499, 305, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11926115804643773, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.1952618956565857}, {"id": 66, "seek": 43600, "start": 440.0, "end": 444.0, "text": " But you can see that it's other choices, we're also fairly sensible.", "tokens": [50564, 583, 291, 393, 536, 300, 309, 311, 661, 7994, 11, 321, 434, 611, 6457, 25380, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11926115804643773, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.1952618956565857}, {"id": 67, "seek": 43600, "start": 444.0, "end": 451.0, "text": " It does look a little bit like a drilling platform. And if you look at its third choice, a lifeboat, it actually looks very like a lifeboat.", "tokens": [50764, 467, 775, 574, 257, 707, 857, 411, 257, 26290, 3663, 13, 400, 498, 291, 574, 412, 1080, 2636, 3922, 11, 257, 993, 31883, 11, 309, 767, 1542, 588, 411, 257, 993, 31883, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11926115804643773, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.1952618956565857}, {"id": 68, "seek": 43600, "start": 451.0, "end": 457.0, "text": " You can see the flag on the front of the boat and the bridge of the boat and the flag at the back and the high surf in the background.", "tokens": [51114, 509, 393, 536, 264, 7166, 322, 264, 1868, 295, 264, 6582, 293, 264, 7283, 295, 264, 6582, 293, 264, 7166, 412, 264, 646, 293, 264, 1090, 9684, 294, 264, 3678, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11926115804643773, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.1952618956565857}, {"id": 69, "seek": 43600, "start": 457.0, "end": 462.0, "text": " So it's, it's errors tell you a lot about how it's doing it and they're very plausible errors.", "tokens": [51414, 407, 309, 311, 11, 309, 311, 13603, 980, 291, 257, 688, 466, 577, 309, 311, 884, 309, 293, 436, 434, 588, 39925, 13603, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11926115804643773, "compression_ratio": 1.855595667870036, "no_speech_prob": 0.1952618956565857}, {"id": 70, "seek": 46200, "start": 462.0, "end": 473.0, "text": " If you look on the left, it gets it wrong, possibly because the beak of the bird is missing and because the feathers of the bird look very like the wet fur of an auto.", "tokens": [50364, 759, 291, 574, 322, 264, 1411, 11, 309, 2170, 309, 2085, 11, 6264, 570, 264, 48663, 295, 264, 5255, 307, 5361, 293, 570, 264, 27044, 295, 264, 5255, 574, 588, 411, 264, 6630, 2687, 295, 364, 8399, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1610789400465945, "compression_ratio": 1.7087378640776698, "no_speech_prob": 0.10465346276760101}, {"id": 71, "seek": 46200, "start": 473.0, "end": 480.0, "text": " But it gets in in this top five and it does better than me. I wouldn't know if that was a quail or a roughed grass or a partridge.", "tokens": [50914, 583, 309, 2170, 294, 294, 341, 1192, 1732, 293, 309, 775, 1101, 813, 385, 13, 286, 2759, 380, 458, 498, 300, 390, 257, 421, 864, 420, 257, 5903, 292, 8054, 420, 257, 644, 15804, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1610789400465945, "compression_ratio": 1.7087378640776698, "no_speech_prob": 0.10465346276760101}, {"id": 72, "seek": 46200, "start": 480.0, "end": 483.0, "text": " If you look on the right, it gets a completely wrong.", "tokens": [51264, 759, 291, 574, 322, 264, 558, 11, 309, 2170, 257, 2584, 2085, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1610789400465945, "compression_ratio": 1.7087378640776698, "no_speech_prob": 0.10465346276760101}, {"id": 73, "seek": 48300, "start": 484.0, "end": 493.0, "text": " It, um, agility, you can see why it says that. You can possibly see why it says orangutan because of the sort of jungle looking background is something orange in the middle.", "tokens": [50414, 467, 11, 1105, 11, 39794, 11, 291, 393, 536, 983, 309, 1619, 300, 13, 509, 393, 6264, 536, 983, 309, 1619, 17481, 30624, 570, 295, 264, 1333, 295, 18228, 1237, 3678, 307, 746, 7671, 294, 264, 2808, 13, 50864], "temperature": 0.0, "avg_logprob": -0.24342513352297665, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.07390002906322479}, {"id": 74, "seek": 48300, "start": 493.0, "end": 496.0, "text": " But it fails to get the right answer.", "tokens": [50864, 583, 309, 18199, 281, 483, 264, 558, 1867, 13, 51014], "temperature": 0.0, "avg_logprob": -0.24342513352297665, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.07390002906322479}, {"id": 75, "seek": 48300, "start": 496.0, "end": 505.0, "text": " It can, however, deal with a wide range of different objects. If you look on the left, I would have said micro-overs, my first answer.", "tokens": [51014, 467, 393, 11, 4461, 11, 2028, 365, 257, 4874, 3613, 295, 819, 6565, 13, 759, 291, 574, 322, 264, 1411, 11, 286, 576, 362, 848, 4532, 12, 78, 840, 11, 452, 700, 1867, 13, 51464], "temperature": 0.0, "avg_logprob": -0.24342513352297665, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.07390002906322479}, {"id": 76, "seek": 50500, "start": 505.0, "end": 513.0, "text": " The labels are very systematic. So actually the correct answer there is electric range and does get it in this top five.", "tokens": [50364, 440, 16949, 366, 588, 27249, 13, 407, 767, 264, 3006, 1867, 456, 307, 5210, 3613, 293, 775, 483, 309, 294, 341, 1192, 1732, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1669735336303711, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.09712856262922287}, {"id": 77, "seek": 50500, "start": 513.0, "end": 520.0, "text": " In the middle it's getting a turn star which is distributed object. It can do more than just recognise compact things.", "tokens": [50764, 682, 264, 2808, 309, 311, 1242, 257, 1261, 3543, 597, 307, 12631, 2657, 13, 467, 393, 360, 544, 813, 445, 23991, 14679, 721, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1669735336303711, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.09712856262922287}, {"id": 78, "seek": 50500, "start": 520.0, "end": 525.0, "text": " And it can also deal with pictures as well as real scenes like the bulletproof vest.", "tokens": [51114, 400, 309, 393, 611, 2028, 365, 5242, 382, 731, 382, 957, 8026, 411, 264, 11632, 15690, 15814, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1669735336303711, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.09712856262922287}, {"id": 79, "seek": 50500, "start": 525.0, "end": 532.0, "text": " And it makes some very cool errors. If you look at the image on the left, that's an earphone.", "tokens": [51364, 400, 309, 1669, 512, 588, 1627, 13603, 13, 759, 291, 574, 412, 264, 3256, 322, 264, 1411, 11, 300, 311, 364, 1273, 4977, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1669735336303711, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.09712856262922287}, {"id": 80, "seek": 53200, "start": 532.0, "end": 537.0, "text": " It doesn't get anything like an earphone, but if you look at it as fourth bet, it thinks it's an ant.", "tokens": [50364, 467, 1177, 380, 483, 1340, 411, 364, 1273, 4977, 11, 457, 498, 291, 574, 412, 309, 382, 6409, 778, 11, 309, 7309, 309, 311, 364, 2511, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13586029108019843, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0224264208227396}, {"id": 81, "seek": 53200, "start": 537.0, "end": 544.0, "text": " And to be with you, think that's crazy. But then if you look at it carefully, you can see it's a view from an ant from underneath. The eyes are looking down at you.", "tokens": [50614, 400, 281, 312, 365, 291, 11, 519, 300, 311, 3219, 13, 583, 550, 498, 291, 574, 412, 309, 7500, 11, 291, 393, 536, 309, 311, 257, 1910, 490, 364, 2511, 490, 7223, 13, 440, 2575, 366, 1237, 760, 412, 291, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13586029108019843, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0224264208227396}, {"id": 82, "seek": 53200, "start": 544.0, "end": 550.0, "text": " And you can see the antennae behind it. It's not the kind of view of an ant you'd like to have if you were a green fly.", "tokens": [50964, 400, 291, 393, 536, 264, 24573, 68, 2261, 309, 13, 467, 311, 406, 264, 733, 295, 1910, 295, 364, 2511, 291, 1116, 411, 281, 362, 498, 291, 645, 257, 3092, 3603, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13586029108019843, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0224264208227396}, {"id": 83, "seek": 53200, "start": 550.0, "end": 559.0, "text": " If you look at the one on the right, it doesn't get the right answer, but all of his answers are cylindrical objects.", "tokens": [51264, 759, 291, 574, 412, 264, 472, 322, 264, 558, 11, 309, 1177, 380, 483, 264, 558, 1867, 11, 457, 439, 295, 702, 6338, 366, 28044, 15888, 6565, 13, 51714], "temperature": 0.0, "avg_logprob": -0.13586029108019843, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0224264208227396}, {"id": 84, "seek": 55900, "start": 560.0, "end": 565.0, "text": " Another task that you're on that's an I very good at is speech recognition.", "tokens": [50414, 3996, 5633, 300, 291, 434, 322, 300, 311, 364, 286, 588, 665, 412, 307, 6218, 11150, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1428179446561837, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.1383487582206726}, {"id": 85, "seek": 55900, "start": 565.0, "end": 568.0, "text": " Or at least part of a speech recognition system.", "tokens": [50664, 1610, 412, 1935, 644, 295, 257, 6218, 11150, 1185, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1428179446561837, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.1383487582206726}, {"id": 86, "seek": 55900, "start": 568.0, "end": 572.0, "text": " So speech recognition systems have several stages.", "tokens": [50814, 407, 6218, 11150, 3652, 362, 2940, 10232, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1428179446561837, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.1383487582206726}, {"id": 87, "seek": 55900, "start": 572.0, "end": 581.0, "text": " First they pre-process the sound wave to get a vector of acoustic coefficients for each 10 milliseconds of sound wave.", "tokens": [51014, 2386, 436, 659, 12, 41075, 264, 1626, 5772, 281, 483, 257, 8062, 295, 26753, 31994, 337, 1184, 1266, 34184, 295, 1626, 5772, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1428179446561837, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.1383487582206726}, {"id": 88, "seek": 55900, "start": 581.0, "end": 584.0, "text": " And so they get 100 of those vectors per second.", "tokens": [51464, 400, 370, 436, 483, 2319, 295, 729, 18875, 680, 1150, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1428179446561837, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.1383487582206726}, {"id": 89, "seek": 58400, "start": 585.0, "end": 588.0, "text": " They then take a few adjacent vectors of acoustic coefficients.", "tokens": [50414, 814, 550, 747, 257, 1326, 24441, 18875, 295, 26753, 31994, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10085253196187538, "compression_ratio": 1.7336065573770492, "no_speech_prob": 0.044176455587148666}, {"id": 90, "seek": 58400, "start": 588.0, "end": 594.0, "text": " And they need to place bets on which part of which phoneme is being spoken.", "tokens": [50564, 400, 436, 643, 281, 1081, 39922, 322, 597, 644, 295, 597, 30754, 5729, 307, 885, 10759, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10085253196187538, "compression_ratio": 1.7336065573770492, "no_speech_prob": 0.044176455587148666}, {"id": 91, "seek": 58400, "start": 594.0, "end": 602.0, "text": " So they look at this little window and they say in the middle of this window, what do I think the phoneme is and which part of the phoneme is it?", "tokens": [50864, 407, 436, 574, 412, 341, 707, 4910, 293, 436, 584, 294, 264, 2808, 295, 341, 4910, 11, 437, 360, 286, 519, 264, 30754, 5729, 307, 293, 597, 644, 295, 264, 30754, 5729, 307, 309, 30, 51264], "temperature": 0.0, "avg_logprob": -0.10085253196187538, "compression_ratio": 1.7336065573770492, "no_speech_prob": 0.044176455587148666}, {"id": 92, "seek": 58400, "start": 602.0, "end": 608.0, "text": " And a good speech recognition system will have many alternative models for a phoneme.", "tokens": [51264, 400, 257, 665, 6218, 11150, 1185, 486, 362, 867, 8535, 5245, 337, 257, 30754, 5729, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10085253196187538, "compression_ratio": 1.7336065573770492, "no_speech_prob": 0.044176455587148666}, {"id": 93, "seek": 58400, "start": 608.0, "end": 611.0, "text": " And each model it might have three different parts.", "tokens": [51564, 400, 1184, 2316, 309, 1062, 362, 1045, 819, 3166, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10085253196187538, "compression_ratio": 1.7336065573770492, "no_speech_prob": 0.044176455587148666}, {"id": 94, "seek": 61100, "start": 611.0, "end": 616.0, "text": " So it might have many thousands of alternative fragments that it thinks this might be.", "tokens": [50364, 407, 309, 1062, 362, 867, 5383, 295, 8535, 29197, 300, 309, 7309, 341, 1062, 312, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0823351782421733, "compression_ratio": 1.7952380952380953, "no_speech_prob": 0.025659754872322083}, {"id": 95, "seek": 61100, "start": 616.0, "end": 621.0, "text": " And you have to place bets on all those thousands of alternatives.", "tokens": [50614, 400, 291, 362, 281, 1081, 39922, 322, 439, 729, 5383, 295, 20478, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0823351782421733, "compression_ratio": 1.7952380952380953, "no_speech_prob": 0.025659754872322083}, {"id": 96, "seek": 61100, "start": 621.0, "end": 629.0, "text": " And then once you place those bets, you have a decoding stage that does the best job it can of using plausible bets,", "tokens": [50864, 400, 550, 1564, 291, 1081, 729, 39922, 11, 291, 362, 257, 979, 8616, 3233, 300, 775, 264, 1151, 1691, 309, 393, 295, 1228, 39925, 39922, 11, 51264], "temperature": 0.0, "avg_logprob": -0.0823351782421733, "compression_ratio": 1.7952380952380953, "no_speech_prob": 0.025659754872322083}, {"id": 97, "seek": 61100, "start": 629.0, "end": 639.0, "text": " but piecing them together into a sequence of bets that corresponds to the kinds of things that people say.", "tokens": [51264, 457, 1730, 2175, 552, 1214, 666, 257, 8310, 295, 39922, 300, 23249, 281, 264, 3685, 295, 721, 300, 561, 584, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0823351782421733, "compression_ratio": 1.7952380952380953, "no_speech_prob": 0.025659754872322083}, {"id": 98, "seek": 63900, "start": 640.0, "end": 646.0, "text": " Currently deep neural networks pioneered by George Darwin, Abdul Rahman, Muhammad at the University of Toronto,", "tokens": [50414, 19964, 2452, 18161, 9590, 19761, 4073, 538, 7136, 30233, 11, 42591, 17844, 1601, 11, 19360, 412, 264, 3535, 295, 14140, 11, 50714], "temperature": 0.0, "avg_logprob": -0.23204037840937225, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.021026574075222015}, {"id": 99, "seek": 63900, "start": 646.0, "end": 651.0, "text": " are doing better than previous machine learning methods for the acoustic model.", "tokens": [50714, 366, 884, 1101, 813, 3894, 3479, 2539, 7150, 337, 264, 26753, 2316, 13, 50964], "temperature": 0.0, "avg_logprob": -0.23204037840937225, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.021026574075222015}, {"id": 100, "seek": 63900, "start": 651.0, "end": 655.0, "text": " And then I begin to be used in practical systems.", "tokens": [50964, 400, 550, 286, 1841, 281, 312, 1143, 294, 8496, 3652, 13, 51164], "temperature": 0.0, "avg_logprob": -0.23204037840937225, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.021026574075222015}, {"id": 101, "seek": 63900, "start": 655.0, "end": 668.0, "text": " So, Dalam Muhammad developed a system that uses many layers of binary neurons.", "tokens": [51164, 407, 11, 17357, 335, 19360, 4743, 257, 1185, 300, 4960, 867, 7914, 295, 17434, 22027, 13, 51814], "temperature": 0.0, "avg_logprob": -0.23204037840937225, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.021026574075222015}, {"id": 102, "seek": 66800, "start": 668.0, "end": 674.0, "text": " To take some acoustic frames and make bets about the labels.", "tokens": [50364, 1407, 747, 512, 26753, 12083, 293, 652, 39922, 466, 264, 16949, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16346988111439317, "compression_ratio": 1.5669291338582678, "no_speech_prob": 0.06827139854431152}, {"id": 103, "seek": 66800, "start": 674.0, "end": 679.0, "text": " They were doing it on a fairly small database and they are used 183 alternative labels.", "tokens": [50664, 814, 645, 884, 309, 322, 257, 6457, 1359, 8149, 293, 436, 366, 1143, 2443, 18, 8535, 16949, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16346988111439317, "compression_ratio": 1.5669291338582678, "no_speech_prob": 0.06827139854431152}, {"id": 104, "seek": 66800, "start": 679.0, "end": 685.0, "text": " And to get this system to well, they did some pre-training which will be described in the second half of the course.", "tokens": [50914, 400, 281, 483, 341, 1185, 281, 731, 11, 436, 630, 512, 659, 12, 17227, 1760, 597, 486, 312, 7619, 294, 264, 1150, 1922, 295, 264, 1164, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16346988111439317, "compression_ratio": 1.5669291338582678, "no_speech_prob": 0.06827139854431152}, {"id": 105, "seek": 66800, "start": 685.0, "end": 695.0, "text": " After standard post-processing, they got 20.7% error rate on a very standard benchmark, which is kind of like the M-nist for speech.", "tokens": [51214, 2381, 3832, 2183, 12, 41075, 278, 11, 436, 658, 945, 13, 22, 4, 6713, 3314, 322, 257, 588, 3832, 18927, 11, 597, 307, 733, 295, 411, 264, 376, 12, 77, 468, 337, 6218, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16346988111439317, "compression_ratio": 1.5669291338582678, "no_speech_prob": 0.06827139854431152}, {"id": 106, "seek": 69500, "start": 695.0, "end": 701.0, "text": " The best previous result on that benchmark for speaker independent recognition was 24.4%.", "tokens": [50364, 440, 1151, 3894, 1874, 322, 300, 18927, 337, 8145, 6695, 11150, 390, 4022, 13, 19, 6856, 50664], "temperature": 0.0, "avg_logprob": -0.21807745297749836, "compression_ratio": 1.51, "no_speech_prob": 0.017747726291418076}, {"id": 107, "seek": 69500, "start": 701.0, "end": 714.0, "text": " And the very experience for speech researcher at Microsoft Research realized that that was a big enough improvement that probably this would change the way speech recognition systems were done.", "tokens": [50664, 400, 264, 588, 1752, 337, 6218, 21751, 412, 8116, 10303, 5334, 300, 300, 390, 257, 955, 1547, 10444, 300, 1391, 341, 576, 1319, 264, 636, 6218, 11150, 3652, 645, 1096, 13, 51314], "temperature": 0.0, "avg_logprob": -0.21807745297749836, "compression_ratio": 1.51, "no_speech_prob": 0.017747726291418076}, {"id": 108, "seek": 69500, "start": 714.0, "end": 716.0, "text": " And indeed it has.", "tokens": [51314, 400, 6451, 309, 575, 13, 51414], "temperature": 0.0, "avg_logprob": -0.21807745297749836, "compression_ratio": 1.51, "no_speech_prob": 0.017747726291418076}, {"id": 109, "seek": 71600, "start": 717.0, "end": 727.0, "text": " So, if you look at recent results from several different leading speech groups, Microsoft showed that this kind of deep neural network,", "tokens": [50414, 407, 11, 498, 291, 574, 412, 5162, 3542, 490, 2940, 819, 5775, 6218, 3935, 11, 8116, 4712, 300, 341, 733, 295, 2452, 18161, 3209, 11, 50914], "temperature": 0.0, "avg_logprob": -0.1207163008776578, "compression_ratio": 1.5118110236220472, "no_speech_prob": 0.31190210580825806}, {"id": 110, "seek": 71600, "start": 727.0, "end": 734.0, "text": " when used as the acoustic model in a speech system, reduced the error rate from 37.4% to 18.5%.", "tokens": [50914, 562, 1143, 382, 264, 26753, 2316, 294, 257, 6218, 1185, 11, 9212, 264, 6713, 3314, 490, 13435, 13, 19, 4, 281, 2443, 13, 20, 6856, 51264], "temperature": 0.0, "avg_logprob": -0.1207163008776578, "compression_ratio": 1.5118110236220472, "no_speech_prob": 0.31190210580825806}, {"id": 111, "seek": 71600, "start": 734.0, "end": 744.0, "text": " Or alternatively, you could view it as reducing the amount of training data you needed from 2000 hours down to 3009 hours to get comparable performance.", "tokens": [51264, 1610, 8535, 356, 11, 291, 727, 1910, 309, 382, 12245, 264, 2372, 295, 3097, 1412, 291, 2978, 490, 8132, 2496, 760, 281, 6641, 24, 2496, 281, 483, 25323, 3389, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1207163008776578, "compression_ratio": 1.5118110236220472, "no_speech_prob": 0.31190210580825806}, {"id": 112, "seek": 74400, "start": 744.0, "end": 762.0, "text": " IBM, which has the best system for one of the standard speech recognition tasks for large recovery speech recognition, showed that even its very highly tuned system that was getting 18.8% can be beaten by one of these deep neural networks.", "tokens": [50364, 23487, 11, 597, 575, 264, 1151, 1185, 337, 472, 295, 264, 3832, 6218, 11150, 9608, 337, 2416, 8597, 6218, 11150, 11, 4712, 300, 754, 1080, 588, 5405, 10870, 1185, 300, 390, 1242, 2443, 13, 23, 4, 393, 312, 17909, 538, 472, 295, 613, 2452, 18161, 9590, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12959046681722006, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.024305304512381554}, {"id": 113, "seek": 74400, "start": 762.0, "end": 769.0, "text": " And Google, fairly recently, trained deep neural network on a large amount of speech, 5,800 hours.", "tokens": [51264, 400, 3329, 11, 6457, 3938, 11, 8895, 2452, 18161, 3209, 322, 257, 2416, 2372, 295, 6218, 11, 1025, 11, 14423, 2496, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12959046681722006, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.024305304512381554}, {"id": 114, "seek": 76900, "start": 769.0, "end": 773.0, "text": " That was still much less than their trained, they got to in mixumod along.", "tokens": [50364, 663, 390, 920, 709, 1570, 813, 641, 8895, 11, 436, 658, 281, 294, 2890, 449, 378, 2051, 13, 50564], "temperature": 0.0, "avg_logprob": -0.20156473101991595, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.09986399114131927}, {"id": 115, "seek": 76900, "start": 773.0, "end": 779.0, "text": " But even with much less data, it did a lot better than the technology they had before.", "tokens": [50564, 583, 754, 365, 709, 1570, 1412, 11, 309, 630, 257, 688, 1101, 813, 264, 2899, 436, 632, 949, 13, 50864], "temperature": 0.0, "avg_logprob": -0.20156473101991595, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.09986399114131927}, {"id": 116, "seek": 76900, "start": 779.0, "end": 784.0, "text": " So, it reduced the error rate from 16% to 12.3% and the error rate is still falling.", "tokens": [50864, 407, 11, 309, 9212, 264, 6713, 3314, 490, 3165, 4, 281, 2272, 13, 18, 4, 293, 264, 6713, 3314, 307, 920, 7440, 13, 51114], "temperature": 0.0, "avg_logprob": -0.20156473101991595, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.09986399114131927}, {"id": 117, "seek": 76900, "start": 784.0, "end": 794.0, "text": " And in the latest Android, if you do voice search, it's using one of these deep neural networks in order to do very good speech recognition.", "tokens": [51114, 400, 294, 264, 6792, 8853, 11, 498, 291, 360, 3177, 3164, 11, 309, 311, 1228, 472, 295, 613, 2452, 18161, 9590, 294, 1668, 281, 360, 588, 665, 6218, 11150, 13, 51614], "temperature": 0.0, "avg_logprob": -0.20156473101991595, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.09986399114131927}], "language": "en"}