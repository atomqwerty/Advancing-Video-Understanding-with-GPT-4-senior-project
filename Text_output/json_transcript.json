{"text": " Hello. Welcome to the Coursera course on neural networks for machine learning. Before we get into the details of neural network learning algorithms, I want to talk a little bit about machine learning, why we need machine learning, the kinds of things we use it for, and show you some examples of what it can do. So, the reason we need machine learning is that there's some problems where it's very hard to write the programs. Recognizing a three-dimensional object, for example, when it's from a novel viewpoint in new lighting conditions in a cluttered scene is very hard to do. We don't know what program to write because we don't know how it's done in our brain. And even if we did know what program to write, it might be that it was a horrendously complicated program. Another example is detecting a fraudulent credit card transaction, where there may not be any nice simple rules that will tell you it's fraudulent. You really need to combine a very large number of not very reliable rules. And also, those rules change over time because people change the tricks they use for fraud. So, we need a complicated program that combines unreliable rules and that we can change easily. The machine learning approach is to say instead of writing each program by hand for each specific task, for a particular task we'll collect a lot of examples that specify the correct output for a given input. A machine learning algorithm then takes these examples and produces a program that does the job. The program produced by the learning algorithm may look very different from a typical handwritten program. For example, it might contain millions of numbers about how you weight different kinds of evidence. If we do it right, the program should work for new cases as well as the ones it's trained on. And if the data changes, it might be that the program is not working for new cases as well as the ones it's trained on. So, if the data changes, we should be able to change the program relatively easily by retraining it on the new data. And now, massive amounts of computation are cheaper than paying someone to write a program for a specific task. So, we can afford big complicated machine learning programs to produce these task specific systems for us. Some examples of the things that are best done by using a learning algorithm are recognizing data. There's also recognizing patterns. So, for example, objects in real scenes. Or the identities or expressions of people's faces. Or spoken words. There's also recognizing anomalies. So, an unusual sequence of credit card transactions would be an anomaly. Another example of an anomaly would be an unusual pattern of sensor readings in a nuclear power plant. And you wouldn't really want to have to deal with those by doing sequence analysis. And then there's supervised learning where you look at the ones that blow up and see what, what caused them to blow up. You'd really like to recognize that something funny is happening without having any supervision signal. It's just not behaving in its normal way. And then there's prediction. So, typically predicting future stock prices or currency exchange rates. Or predicting which movies a person will like from knowing which other movies they like and which movies a lot of other people liked. So, in this course, I'm going to use a standard example for explaining a lot of the machine learning algorithms. This is done in a lot of science. In genetics, for example, a lot of genetics is done on fruit flies. And the reason is they're convenient. They breed fast. And a lot is already known about the genetics of fruit flies. The MNIST database of handwritten digits is the machine learning algorithm that is the machine learning equivalent of fruit flies. It's publicly available. We can get machine learning algorithms to learn how to recognize these handwritten digits quite quickly. So, it's easy to try lots of variations. And we know huge amounts about how well different machine learning methods do on MNIST. And in particular, the different machine learning methods were implemented by people who believed in them. So, we can rely on those results. So, for all those reasons, we're going to use MNIST as our standard task. Here's an example of some of the digits in MNIST. These are ones that were correctly recognized by neural net the first time it saw them. But they're ones where the neural net wasn't very confident. And you can see why. I've arranged these digits in standard scanline order. So, zeros, then ones, then twos, and so on. If you look at a bunch of twos, like the ones in the green rectangle, you can see that if you knew they were a handwritten digit, you'd probably guess they were twos. But it's very hard to say what it is that makes them twos. There's nothing simple that they all have in common. In particular, if you try and overlay one on another, you'll see it doesn't fit. And even if you skew it a bit, it's very hard to make them overlay on each other. So, a template isn't going to do the job. And in particular, if you try and overlay one on another, you'll see it doesn't fit. And in particular, a template is going to be very hard to find that'll fit those twos in the green box and won't also fit the things in the red boxes. So, that's one thing that makes recognizing handwritten digits a good task for machine learning. Now, I don't want you to think that's the only thing we can do. It's a relatively simple thing for a machine learning system to do now. And to motivate the rest of the course, I want to show you some examples of much more difficult things. So, we now have neural nets with approaching 100 million parameters in them that can recognize a thousand different object classes in 1.3 million high resolution training images got from the web. So, there was a competition in 2010 and the best system got 47% error rate if you look at its first choice and 25% error rate if you say it got it right if it was in its top five choices, which isn't bad for a thousand percent error rate. Which isn't bad for a thousand different objects. Jitendra Malik, who's an eminent neural net skeptic and a leading computer vision researcher has said that this competition is a good test of whether deep neural networks can work well for object recognition. And a very deep neural network can now do considerably better than the thing that won the competition. It can get less than 40% error for its first choice and less than 20% error for its top five choices. I'll describe that in much more detail in lecture five. Here's some examples of the kinds of images you have to recognize. These are images from the test set that it's never seen before. And below the examples, I'm showing you what the neural net thought the right answer was, where the length of the horizontal bar is how confident it was, and the correct answer is in red. So, if you look in the middle, it correctly identified that as a snowplow. But you can see that it's unbiased. But you can see that its other choices were also fairly sensible. It does look a little bit like a drilling platform. And if you look at its third choice, a lifeboat, it actually looks very like a lifeboat. You can see the flag on the front of the boat and the bridge of the boat and the flag at the back and the high surf in the background. So it's, its errors tell you a lot about how it's doing it and they're very plausible errors. If you look on the left, it gets it wrong, possibly because the beak of the bird is missing and because the feathers of the bird look very like the flag of the bird. Look very like the wet fur of an otter. But it gets it in its top five and it does better than me. I wouldn't know if that was a quail or a roughed grouse or a partridge. If you look on the right, it gets it completely wrong. It a guillotine, you can see why it says that. You can possibly see why it says orangutan because of the sort of jungle looking background and something orange in the middle. But it fails to get the right answer. It can, however, deal with a wide range of different objects. If you look on the left, I would have said microwave was my first answer. The labels aren't very systematic, so actually the correct answer there is electric range and does get it in its top five. In the middle, it's getting a turnstile, which is a distributed object. It does, it can do more than just recognize compact things. And it can also deal with pictures as well as real scenes, like the bulletproof vest. And it makes some very cool errors. If you look at the image on the left, it's an earphone. It doesn't get anything like an earphone. But if you look at its fourth bet, it thinks it's an ant. And to begin with, you think that's crazy. But then if you look at it carefully, you can see it's a view from an ant from underneath. The eyes are looking down at you, and you can see the antennae behind it. It's not the kind of view of an ant you'd like to have if you were a green fly. If you look at the one on the right, it doesn't get the right answer. But all of its answers are cylindrical objects. Another question that I've been asked is, what is the difference between a sound wave and a speech recognition system? And another task that neural nets are now very good at is speech recognition. Or at least part of a speech recognition system. So speech recognition systems have several stages. First they pre-process the sound wave to get a vector of acoustic coefficients for each 10 milliseconds of sound wave. And so they get a hundred of those vectors per second. They then take a few adjacent vectors of acoustic coefficients, and they need to place bets on which part of which phoneme is being spoken. So they look at this little window, and they say in the middle of this window, what do I think the phoneme is, and which part of the phoneme is it? And a good speech recognition system will have many alternative models for a phoneme. And each model it might have three different parts. So it might have many thousands of alternative fragments that it thinks this might be. And you have to place bets on all those thousands of alternatives. And then once you place those bets, you have a decoding stage that does the best job it can of using plausible bets, but piecing them together into a sequence of bets that corresponds to the kinds of things that people say. Currently, deep neural networks pioneered by George Dahl and Abdul Rahman, Mohammed, at the University of Toronto, are doing better than previous machine learning methods for the economy. They're now using the acoustic model, and they're now beginning to be used in practical systems. So Dahl and Mohammed developed a system that uses many layers of binary neurons to take some acoustic frames and make bets about the labels. They were doing it on a fairly small database, and they only used 183 alternative labels. And to get their system to work well, they did some pre-training, which will be described in the second half of the course. After standard post-processing, they got 20.7% error rate on a very standard benchmark, which is kind of like the MNIST for speech. The best previous result on that benchmark for speaker independent recognition was 24.4%. And a very experienced speech researcher at Microsoft Research realized that that was a big enough improvement that probably this would change the way speech recognition systems were done, and indeed it has. So if you look at recent results from several different leading speech groups, Microsoft showed that this kind of deep neural network, when used as the acoustic model in a speech system, reduced the error rate from 27.4% to 18.5%,Hi, and relatively, you could view it as reducing the amount of training data you needed from 2,000 hours down to 309 hours to get comparable performance. IBM, which has the best system for one of the standard speech recognition tasks for large vocabulary speech recognition, showed that even its very highly tuned system that was getting 18.8% can be beaten by one of these deep neural networks. And Google, fairly recently, trained a deep neural network to do the same. And Google, fairly recently, trained a deep neural network to do the same. And Google, fairly recently, trained a deep neural network to do the same. Google, fairly recently, trained a deep neural network to do the same. And Google, fairly recently, trained a deep neural network to do the same. So for example, in this case it was a news ad that was a little bit less than the average speech recognition time of the day, and even with your big neural network on a large amount of speech, 5,800 hours, that was still much less than they trained their Gaussian mixturing model on. But even with much less data, it did a lot better than the technology they had before. So it reduced the error rate from 16% to 12.3%, and the error rate is still falling. And in the latest Android, if you do voice search, it's using one of these deep neural networks in order to do very good speech recognition.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.0, "text": " Hello. Welcome to the Coursera course on neural", "tokens": [50365, 2425, 13, 4027, 281, 264, 383, 5067, 1663, 1164, 322, 18161, 50565], "temperature": 0.0, "avg_logprob": -0.1325432768145811, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0038178672548383474}, {"id": 1, "seek": 0, "start": 4.0, "end": 9.0, "text": " networks for machine learning. Before we get into the details of neural", "tokens": [50565, 9590, 337, 3479, 2539, 13, 4546, 321, 483, 666, 264, 4365, 295, 18161, 50815], "temperature": 0.0, "avg_logprob": -0.1325432768145811, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0038178672548383474}, {"id": 2, "seek": 0, "start": 9.0, "end": 14.0, "text": " network learning algorithms, I want to talk a little bit about machine learning,", "tokens": [50815, 3209, 2539, 14642, 11, 286, 528, 281, 751, 257, 707, 857, 466, 3479, 2539, 11, 51065], "temperature": 0.0, "avg_logprob": -0.1325432768145811, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0038178672548383474}, {"id": 3, "seek": 0, "start": 14.0, "end": 19.0, "text": " why we need machine learning, the kinds of things we use it for, and show you some", "tokens": [51065, 983, 321, 643, 3479, 2539, 11, 264, 3685, 295, 721, 321, 764, 309, 337, 11, 293, 855, 291, 512, 51315], "temperature": 0.0, "avg_logprob": -0.1325432768145811, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0038178672548383474}, {"id": 4, "seek": 0, "start": 19.0, "end": 24.0, "text": " examples of what it can do. So, the reason we need machine learning is", "tokens": [51315, 5110, 295, 437, 309, 393, 360, 13, 407, 11, 264, 1778, 321, 643, 3479, 2539, 307, 51565], "temperature": 0.0, "avg_logprob": -0.1325432768145811, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0038178672548383474}, {"id": 5, "seek": 0, "start": 24.0, "end": 29.0, "text": " that there's some problems where it's very hard to write the programs.", "tokens": [51565, 300, 456, 311, 512, 2740, 689, 309, 311, 588, 1152, 281, 2464, 264, 4268, 13, 51815], "temperature": 0.0, "avg_logprob": -0.1325432768145811, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0038178672548383474}, {"id": 6, "seek": 2900, "start": 29.0, "end": 33.0, "text": " Recognizing a three-dimensional object, for example, when it's from a novel", "tokens": [50365, 44682, 3319, 257, 1045, 12, 18759, 2657, 11, 337, 1365, 11, 562, 309, 311, 490, 257, 7613, 50565], "temperature": 0.0, "avg_logprob": -0.07548271525989879, "compression_ratio": 1.721254355400697, "no_speech_prob": 8.475989488943014e-06}, {"id": 7, "seek": 2900, "start": 33.0, "end": 38.0, "text": " viewpoint in new lighting conditions in a cluttered scene is very hard to do.", "tokens": [50565, 35248, 294, 777, 9577, 4487, 294, 257, 40614, 292, 4145, 307, 588, 1152, 281, 360, 13, 50815], "temperature": 0.0, "avg_logprob": -0.07548271525989879, "compression_ratio": 1.721254355400697, "no_speech_prob": 8.475989488943014e-06}, {"id": 8, "seek": 2900, "start": 38.0, "end": 42.0, "text": " We don't know what program to write because we don't know how it's done in", "tokens": [50815, 492, 500, 380, 458, 437, 1461, 281, 2464, 570, 321, 500, 380, 458, 577, 309, 311, 1096, 294, 51015], "temperature": 0.0, "avg_logprob": -0.07548271525989879, "compression_ratio": 1.721254355400697, "no_speech_prob": 8.475989488943014e-06}, {"id": 9, "seek": 2900, "start": 42.0, "end": 45.0, "text": " our brain. And even if we did know what program to", "tokens": [51015, 527, 3567, 13, 400, 754, 498, 321, 630, 458, 437, 1461, 281, 51165], "temperature": 0.0, "avg_logprob": -0.07548271525989879, "compression_ratio": 1.721254355400697, "no_speech_prob": 8.475989488943014e-06}, {"id": 10, "seek": 2900, "start": 45.0, "end": 50.0, "text": " write, it might be that it was a horrendously complicated program.", "tokens": [51165, 2464, 11, 309, 1062, 312, 300, 309, 390, 257, 49520, 5098, 6179, 1461, 13, 51415], "temperature": 0.0, "avg_logprob": -0.07548271525989879, "compression_ratio": 1.721254355400697, "no_speech_prob": 8.475989488943014e-06}, {"id": 11, "seek": 2900, "start": 50.0, "end": 55.0, "text": " Another example is detecting a fraudulent credit card transaction, where there may", "tokens": [51415, 3996, 1365, 307, 40237, 257, 14560, 23405, 5397, 2920, 14425, 11, 689, 456, 815, 51665], "temperature": 0.0, "avg_logprob": -0.07548271525989879, "compression_ratio": 1.721254355400697, "no_speech_prob": 8.475989488943014e-06}, {"id": 12, "seek": 2900, "start": 55.0, "end": 59.0, "text": " not be any nice simple rules that will tell you it's fraudulent.", "tokens": [51665, 406, 312, 604, 1481, 2199, 4474, 300, 486, 980, 291, 309, 311, 14560, 23405, 13, 51865], "temperature": 0.0, "avg_logprob": -0.07548271525989879, "compression_ratio": 1.721254355400697, "no_speech_prob": 8.475989488943014e-06}, {"id": 13, "seek": 5900, "start": 59.0, "end": 65.0, "text": " You really need to combine a very large number of not very reliable rules.", "tokens": [50365, 509, 534, 643, 281, 10432, 257, 588, 2416, 1230, 295, 406, 588, 12924, 4474, 13, 50665], "temperature": 0.0, "avg_logprob": -0.09748629933780002, "compression_ratio": 1.6732283464566928, "no_speech_prob": 1.6532345398445614e-05}, {"id": 14, "seek": 5900, "start": 65.0, "end": 70.0, "text": " And also, those rules change over time because people change the tricks they use", "tokens": [50665, 400, 611, 11, 729, 4474, 1319, 670, 565, 570, 561, 1319, 264, 11733, 436, 764, 50915], "temperature": 0.0, "avg_logprob": -0.09748629933780002, "compression_ratio": 1.6732283464566928, "no_speech_prob": 1.6532345398445614e-05}, {"id": 15, "seek": 5900, "start": 70.0, "end": 73.0, "text": " for fraud. So, we need a complicated program that", "tokens": [50915, 337, 14560, 13, 407, 11, 321, 643, 257, 6179, 1461, 300, 51065], "temperature": 0.0, "avg_logprob": -0.09748629933780002, "compression_ratio": 1.6732283464566928, "no_speech_prob": 1.6532345398445614e-05}, {"id": 16, "seek": 5900, "start": 73.0, "end": 77.0, "text": " combines unreliable rules and that we can change easily.", "tokens": [51065, 29520, 20584, 2081, 712, 4474, 293, 300, 321, 393, 1319, 3612, 13, 51265], "temperature": 0.0, "avg_logprob": -0.09748629933780002, "compression_ratio": 1.6732283464566928, "no_speech_prob": 1.6532345398445614e-05}, {"id": 17, "seek": 5900, "start": 77.0, "end": 83.0, "text": " The machine learning approach is to say instead of writing each program by hand", "tokens": [51265, 440, 3479, 2539, 3109, 307, 281, 584, 2602, 295, 3579, 1184, 1461, 538, 1011, 51565], "temperature": 0.0, "avg_logprob": -0.09748629933780002, "compression_ratio": 1.6732283464566928, "no_speech_prob": 1.6532345398445614e-05}, {"id": 18, "seek": 5900, "start": 83.0, "end": 88.0, "text": " for each specific task, for a particular task we'll collect a lot of examples that", "tokens": [51565, 337, 1184, 2685, 5633, 11, 337, 257, 1729, 5633, 321, 603, 2500, 257, 688, 295, 5110, 300, 51815], "temperature": 0.0, "avg_logprob": -0.09748629933780002, "compression_ratio": 1.6732283464566928, "no_speech_prob": 1.6532345398445614e-05}, {"id": 19, "seek": 8900, "start": 89.0, "end": 92.0, "text": " specify the correct output for a given input.", "tokens": [50365, 16500, 264, 3006, 5598, 337, 257, 2212, 4846, 13, 50515], "temperature": 0.0, "avg_logprob": -0.20803227424621581, "compression_ratio": 1.7891156462585034, "no_speech_prob": 1.1855810953420587e-05}, {"id": 20, "seek": 8900, "start": 92.0, "end": 97.0, "text": " A machine learning algorithm then takes these examples and produces a program that", "tokens": [50515, 316, 3479, 2539, 9284, 550, 2516, 613, 5110, 293, 14725, 257, 1461, 300, 50765], "temperature": 0.0, "avg_logprob": -0.20803227424621581, "compression_ratio": 1.7891156462585034, "no_speech_prob": 1.1855810953420587e-05}, {"id": 21, "seek": 8900, "start": 97.0, "end": 101.0, "text": " does the job. The program produced by the learning", "tokens": [50765, 775, 264, 1691, 13, 440, 1461, 7126, 538, 264, 2539, 50965], "temperature": 0.0, "avg_logprob": -0.20803227424621581, "compression_ratio": 1.7891156462585034, "no_speech_prob": 1.1855810953420587e-05}, {"id": 22, "seek": 8900, "start": 101.0, "end": 106.0, "text": " algorithm may look very different from a typical handwritten program.", "tokens": [50965, 9284, 815, 574, 588, 819, 490, 257, 7476, 1011, 26859, 1461, 13, 51215], "temperature": 0.0, "avg_logprob": -0.20803227424621581, "compression_ratio": 1.7891156462585034, "no_speech_prob": 1.1855810953420587e-05}, {"id": 23, "seek": 8900, "start": 106.0, "end": 109.0, "text": " For example, it might contain millions of numbers about how you weight different", "tokens": [51215, 1171, 1365, 11, 309, 1062, 5304, 6803, 295, 3547, 466, 577, 291, 3364, 819, 51365], "temperature": 0.0, "avg_logprob": -0.20803227424621581, "compression_ratio": 1.7891156462585034, "no_speech_prob": 1.1855810953420587e-05}, {"id": 24, "seek": 8900, "start": 109.0, "end": 114.0, "text": " kinds of evidence. If we do it right, the program should work", "tokens": [51365, 3685, 295, 4467, 13, 759, 321, 360, 309, 558, 11, 264, 1461, 820, 589, 51615], "temperature": 0.0, "avg_logprob": -0.20803227424621581, "compression_ratio": 1.7891156462585034, "no_speech_prob": 1.1855810953420587e-05}, {"id": 25, "seek": 8900, "start": 114.0, "end": 117.0, "text": " for new cases as well as the ones it's trained on.", "tokens": [51615, 337, 777, 3331, 382, 731, 382, 264, 2306, 309, 311, 8895, 322, 13, 51765], "temperature": 0.0, "avg_logprob": -0.20803227424621581, "compression_ratio": 1.7891156462585034, "no_speech_prob": 1.1855810953420587e-05}, {"id": 26, "seek": 8900, "start": 117.0, "end": 119.0, "text": " And if the data changes, it might be that the program is not working for new cases", "tokens": [51765, 400, 498, 264, 1412, 2962, 11, 309, 1062, 312, 300, 264, 1461, 307, 406, 1364, 337, 777, 3331, 51865], "temperature": 0.0, "avg_logprob": -0.20803227424621581, "compression_ratio": 1.7891156462585034, "no_speech_prob": 1.1855810953420587e-05}, {"id": 27, "seek": 11900, "start": 119.0, "end": 122.0, "text": " as well as the ones it's trained on. So, if the data changes, we should be able", "tokens": [50365, 382, 731, 382, 264, 2306, 309, 311, 8895, 322, 13, 407, 11, 498, 264, 1412, 2962, 11, 321, 820, 312, 1075, 50515], "temperature": 0.0, "avg_logprob": -0.2020845325714951, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.3268984730530065e-05}, {"id": 28, "seek": 11900, "start": 122.0, "end": 127.0, "text": " to change the program relatively easily by retraining it on the new data.", "tokens": [50515, 281, 1319, 264, 1461, 7226, 3612, 538, 49356, 1760, 309, 322, 264, 777, 1412, 13, 50765], "temperature": 0.0, "avg_logprob": -0.2020845325714951, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.3268984730530065e-05}, {"id": 29, "seek": 11900, "start": 127.0, "end": 132.0, "text": " And now, massive amounts of computation are cheaper than paying someone to write a", "tokens": [50765, 400, 586, 11, 5994, 11663, 295, 24903, 366, 12284, 813, 6229, 1580, 281, 2464, 257, 51015], "temperature": 0.0, "avg_logprob": -0.2020845325714951, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.3268984730530065e-05}, {"id": 30, "seek": 11900, "start": 132.0, "end": 137.0, "text": " program for a specific task. So, we can afford big complicated machine", "tokens": [51015, 1461, 337, 257, 2685, 5633, 13, 407, 11, 321, 393, 6157, 955, 6179, 3479, 51265], "temperature": 0.0, "avg_logprob": -0.2020845325714951, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.3268984730530065e-05}, {"id": 31, "seek": 11900, "start": 137.0, "end": 143.0, "text": " learning programs to produce these task specific systems for us.", "tokens": [51265, 2539, 4268, 281, 5258, 613, 5633, 2685, 3652, 337, 505, 13, 51565], "temperature": 0.0, "avg_logprob": -0.2020845325714951, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.3268984730530065e-05}, {"id": 32, "seek": 11900, "start": 143.0, "end": 147.0, "text": " Some examples of the things that are best done by using a learning algorithm are", "tokens": [51565, 2188, 5110, 295, 264, 721, 300, 366, 1151, 1096, 538, 1228, 257, 2539, 9284, 366, 51765], "temperature": 0.0, "avg_logprob": -0.2020845325714951, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.3268984730530065e-05}, {"id": 33, "seek": 11900, "start": 147.0, "end": 149.0, "text": " recognizing data.", "tokens": [51765, 18538, 1412, 13, 51865], "temperature": 0.0, "avg_logprob": -0.2020845325714951, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.3268984730530065e-05}, {"id": 34, "seek": 14900, "start": 149.0, "end": 153.0, "text": " There's also recognizing patterns. So, for example, objects in real scenes.", "tokens": [50365, 821, 311, 611, 18538, 8294, 13, 407, 11, 337, 1365, 11, 6565, 294, 957, 8026, 13, 50565], "temperature": 0.0, "avg_logprob": -0.173218147427428, "compression_ratio": 1.7670682730923695, "no_speech_prob": 0.00013652790221385658}, {"id": 35, "seek": 14900, "start": 153.0, "end": 157.0, "text": " Or the identities or expressions of people's faces.", "tokens": [50565, 1610, 264, 24239, 420, 15277, 295, 561, 311, 8475, 13, 50765], "temperature": 0.0, "avg_logprob": -0.173218147427428, "compression_ratio": 1.7670682730923695, "no_speech_prob": 0.00013652790221385658}, {"id": 36, "seek": 14900, "start": 157.0, "end": 164.0, "text": " Or spoken words. There's also recognizing anomalies.", "tokens": [50765, 1610, 10759, 2283, 13, 821, 311, 611, 18538, 24769, 48872, 13, 51115], "temperature": 0.0, "avg_logprob": -0.173218147427428, "compression_ratio": 1.7670682730923695, "no_speech_prob": 0.00013652790221385658}, {"id": 37, "seek": 14900, "start": 164.0, "end": 169.0, "text": " So, an unusual sequence of credit card transactions would be an anomaly.", "tokens": [51115, 407, 11, 364, 10901, 8310, 295, 5397, 2920, 16856, 576, 312, 364, 42737, 13, 51365], "temperature": 0.0, "avg_logprob": -0.173218147427428, "compression_ratio": 1.7670682730923695, "no_speech_prob": 0.00013652790221385658}, {"id": 38, "seek": 14900, "start": 169.0, "end": 174.0, "text": " Another example of an anomaly would be an unusual pattern of sensor readings in a", "tokens": [51365, 3996, 1365, 295, 364, 42737, 576, 312, 364, 10901, 5102, 295, 10200, 27319, 294, 257, 51615], "temperature": 0.0, "avg_logprob": -0.173218147427428, "compression_ratio": 1.7670682730923695, "no_speech_prob": 0.00013652790221385658}, {"id": 39, "seek": 14900, "start": 174.0, "end": 177.0, "text": " nuclear power plant. And you wouldn't really want to have to", "tokens": [51615, 8179, 1347, 3709, 13, 400, 291, 2759, 380, 534, 528, 281, 362, 281, 51765], "temperature": 0.0, "avg_logprob": -0.173218147427428, "compression_ratio": 1.7670682730923695, "no_speech_prob": 0.00013652790221385658}, {"id": 40, "seek": 14900, "start": 177.0, "end": 179.0, "text": " deal with those by doing sequence analysis.", "tokens": [51765, 2028, 365, 729, 538, 884, 8310, 5215, 13, 51865], "temperature": 0.0, "avg_logprob": -0.173218147427428, "compression_ratio": 1.7670682730923695, "no_speech_prob": 0.00013652790221385658}, {"id": 41, "seek": 17900, "start": 179.0, "end": 182.0, "text": " And then there's supervised learning where you look at the ones that blow up", "tokens": [50365, 400, 550, 456, 311, 46533, 2539, 689, 291, 574, 412, 264, 2306, 300, 6327, 493, 50515], "temperature": 0.0, "avg_logprob": -0.1072700023651123, "compression_ratio": 1.7703180212014133, "no_speech_prob": 4.0075560718833e-06}, {"id": 42, "seek": 17900, "start": 182.0, "end": 186.0, "text": " and see what, what caused them to blow up. You'd really like to recognize that", "tokens": [50515, 293, 536, 437, 11, 437, 7008, 552, 281, 6327, 493, 13, 509, 1116, 534, 411, 281, 5521, 300, 50715], "temperature": 0.0, "avg_logprob": -0.1072700023651123, "compression_ratio": 1.7703180212014133, "no_speech_prob": 4.0075560718833e-06}, {"id": 43, "seek": 17900, "start": 186.0, "end": 190.0, "text": " something funny is happening without having any supervision signal.", "tokens": [50715, 746, 4074, 307, 2737, 1553, 1419, 604, 32675, 6358, 13, 50915], "temperature": 0.0, "avg_logprob": -0.1072700023651123, "compression_ratio": 1.7703180212014133, "no_speech_prob": 4.0075560718833e-06}, {"id": 44, "seek": 17900, "start": 190.0, "end": 195.0, "text": " It's just not behaving in its normal way. And then there's prediction.", "tokens": [50915, 467, 311, 445, 406, 35263, 294, 1080, 2710, 636, 13, 400, 550, 456, 311, 17630, 13, 51165], "temperature": 0.0, "avg_logprob": -0.1072700023651123, "compression_ratio": 1.7703180212014133, "no_speech_prob": 4.0075560718833e-06}, {"id": 45, "seek": 17900, "start": 195.0, "end": 200.0, "text": " So, typically predicting future stock prices or currency exchange rates.", "tokens": [51165, 407, 11, 5850, 32884, 2027, 4127, 7901, 420, 13346, 7742, 6846, 13, 51415], "temperature": 0.0, "avg_logprob": -0.1072700023651123, "compression_ratio": 1.7703180212014133, "no_speech_prob": 4.0075560718833e-06}, {"id": 46, "seek": 17900, "start": 200.0, "end": 204.0, "text": " Or predicting which movies a person will like from knowing which other movies they", "tokens": [51415, 1610, 32884, 597, 6233, 257, 954, 486, 411, 490, 5276, 597, 661, 6233, 436, 51615], "temperature": 0.0, "avg_logprob": -0.1072700023651123, "compression_ratio": 1.7703180212014133, "no_speech_prob": 4.0075560718833e-06}, {"id": 47, "seek": 17900, "start": 204.0, "end": 207.0, "text": " like and which movies a lot of other people liked.", "tokens": [51615, 411, 293, 597, 6233, 257, 688, 295, 661, 561, 4501, 13, 51765], "temperature": 0.0, "avg_logprob": -0.1072700023651123, "compression_ratio": 1.7703180212014133, "no_speech_prob": 4.0075560718833e-06}, {"id": 48, "seek": 20700, "start": 207.0, "end": 213.0, "text": " So, in this course, I'm going to use a standard example for explaining a lot of", "tokens": [50365, 407, 11, 294, 341, 1164, 11, 286, 478, 516, 281, 764, 257, 3832, 1365, 337, 13468, 257, 688, 295, 50665], "temperature": 0.0, "avg_logprob": -0.11297424166810278, "compression_ratio": 1.748936170212766, "no_speech_prob": 5.013978352508275e-06}, {"id": 49, "seek": 20700, "start": 213.0, "end": 218.0, "text": " the machine learning algorithms. This is done in a lot of science.", "tokens": [50665, 264, 3479, 2539, 14642, 13, 639, 307, 1096, 294, 257, 688, 295, 3497, 13, 50915], "temperature": 0.0, "avg_logprob": -0.11297424166810278, "compression_ratio": 1.748936170212766, "no_speech_prob": 5.013978352508275e-06}, {"id": 50, "seek": 20700, "start": 218.0, "end": 223.0, "text": " In genetics, for example, a lot of genetics is done on fruit flies.", "tokens": [50915, 682, 26516, 11, 337, 1365, 11, 257, 688, 295, 26516, 307, 1096, 322, 6773, 17414, 13, 51165], "temperature": 0.0, "avg_logprob": -0.11297424166810278, "compression_ratio": 1.748936170212766, "no_speech_prob": 5.013978352508275e-06}, {"id": 51, "seek": 20700, "start": 223.0, "end": 227.0, "text": " And the reason is they're convenient. They breed fast.", "tokens": [51165, 400, 264, 1778, 307, 436, 434, 10851, 13, 814, 18971, 2370, 13, 51365], "temperature": 0.0, "avg_logprob": -0.11297424166810278, "compression_ratio": 1.748936170212766, "no_speech_prob": 5.013978352508275e-06}, {"id": 52, "seek": 20700, "start": 227.0, "end": 231.0, "text": " And a lot is already known about the genetics of fruit flies.", "tokens": [51365, 400, 257, 688, 307, 1217, 2570, 466, 264, 26516, 295, 6773, 17414, 13, 51565], "temperature": 0.0, "avg_logprob": -0.11297424166810278, "compression_ratio": 1.748936170212766, "no_speech_prob": 5.013978352508275e-06}, {"id": 53, "seek": 20700, "start": 231.0, "end": 237.0, "text": " The MNIST database of handwritten digits is the machine learning algorithm that", "tokens": [51565, 440, 376, 45, 19756, 8149, 295, 1011, 26859, 27011, 307, 264, 3479, 2539, 9284, 300, 51865], "temperature": 0.0, "avg_logprob": -0.11297424166810278, "compression_ratio": 1.748936170212766, "no_speech_prob": 5.013978352508275e-06}, {"id": 54, "seek": 23700, "start": 237.0, "end": 240.0, "text": " is the machine learning equivalent of fruit flies.", "tokens": [50365, 307, 264, 3479, 2539, 10344, 295, 6773, 17414, 13, 50515], "temperature": 0.0, "avg_logprob": -0.10638855970822848, "compression_ratio": 1.7110266159695817, "no_speech_prob": 4.712193003797438e-06}, {"id": 55, "seek": 23700, "start": 240.0, "end": 245.0, "text": " It's publicly available. We can get machine learning algorithms to", "tokens": [50515, 467, 311, 14843, 2435, 13, 492, 393, 483, 3479, 2539, 14642, 281, 50765], "temperature": 0.0, "avg_logprob": -0.10638855970822848, "compression_ratio": 1.7110266159695817, "no_speech_prob": 4.712193003797438e-06}, {"id": 56, "seek": 23700, "start": 245.0, "end": 249.0, "text": " learn how to recognize these handwritten digits quite quickly.", "tokens": [50765, 1466, 577, 281, 5521, 613, 1011, 26859, 27011, 1596, 2661, 13, 50965], "temperature": 0.0, "avg_logprob": -0.10638855970822848, "compression_ratio": 1.7110266159695817, "no_speech_prob": 4.712193003797438e-06}, {"id": 57, "seek": 23700, "start": 249.0, "end": 254.0, "text": " So, it's easy to try lots of variations. And we know huge amounts about how well", "tokens": [50965, 407, 11, 309, 311, 1858, 281, 853, 3195, 295, 17840, 13, 400, 321, 458, 2603, 11663, 466, 577, 731, 51215], "temperature": 0.0, "avg_logprob": -0.10638855970822848, "compression_ratio": 1.7110266159695817, "no_speech_prob": 4.712193003797438e-06}, {"id": 58, "seek": 23700, "start": 254.0, "end": 257.0, "text": " different machine learning methods do on MNIST.", "tokens": [51215, 819, 3479, 2539, 7150, 360, 322, 376, 45, 19756, 13, 51365], "temperature": 0.0, "avg_logprob": -0.10638855970822848, "compression_ratio": 1.7110266159695817, "no_speech_prob": 4.712193003797438e-06}, {"id": 59, "seek": 23700, "start": 257.0, "end": 261.0, "text": " And in particular, the different machine learning methods were implemented by", "tokens": [51365, 400, 294, 1729, 11, 264, 819, 3479, 2539, 7150, 645, 12270, 538, 51565], "temperature": 0.0, "avg_logprob": -0.10638855970822848, "compression_ratio": 1.7110266159695817, "no_speech_prob": 4.712193003797438e-06}, {"id": 60, "seek": 23700, "start": 261.0, "end": 265.0, "text": " people who believed in them. So, we can rely on those results.", "tokens": [51565, 561, 567, 7847, 294, 552, 13, 407, 11, 321, 393, 10687, 322, 729, 3542, 13, 51765], "temperature": 0.0, "avg_logprob": -0.10638855970822848, "compression_ratio": 1.7110266159695817, "no_speech_prob": 4.712193003797438e-06}, {"id": 61, "seek": 26500, "start": 265.0, "end": 269.0, "text": " So, for all those reasons, we're going to use MNIST as our standard task.", "tokens": [50365, 407, 11, 337, 439, 729, 4112, 11, 321, 434, 516, 281, 764, 376, 45, 19756, 382, 527, 3832, 5633, 13, 50565], "temperature": 0.0, "avg_logprob": -0.09287765196391515, "compression_ratio": 1.6, "no_speech_prob": 4.7604949031665456e-06}, {"id": 62, "seek": 26500, "start": 269.0, "end": 274.0, "text": " Here's an example of some of the digits in MNIST.", "tokens": [50565, 1692, 311, 364, 1365, 295, 512, 295, 264, 27011, 294, 376, 45, 19756, 13, 50815], "temperature": 0.0, "avg_logprob": -0.09287765196391515, "compression_ratio": 1.6, "no_speech_prob": 4.7604949031665456e-06}, {"id": 63, "seek": 26500, "start": 274.0, "end": 279.0, "text": " These are ones that were correctly recognized by neural net the first time it", "tokens": [50815, 1981, 366, 2306, 300, 645, 8944, 9823, 538, 18161, 2533, 264, 700, 565, 309, 51065], "temperature": 0.0, "avg_logprob": -0.09287765196391515, "compression_ratio": 1.6, "no_speech_prob": 4.7604949031665456e-06}, {"id": 64, "seek": 26500, "start": 279.0, "end": 283.0, "text": " saw them. But they're ones where the neural net", "tokens": [51065, 1866, 552, 13, 583, 436, 434, 2306, 689, 264, 18161, 2533, 51265], "temperature": 0.0, "avg_logprob": -0.09287765196391515, "compression_ratio": 1.6, "no_speech_prob": 4.7604949031665456e-06}, {"id": 65, "seek": 26500, "start": 283.0, "end": 286.0, "text": " wasn't very confident. And you can see why.", "tokens": [51265, 2067, 380, 588, 6679, 13, 400, 291, 393, 536, 983, 13, 51415], "temperature": 0.0, "avg_logprob": -0.09287765196391515, "compression_ratio": 1.6, "no_speech_prob": 4.7604949031665456e-06}, {"id": 66, "seek": 26500, "start": 286.0, "end": 291.0, "text": " I've arranged these digits in standard scanline order.", "tokens": [51415, 286, 600, 18721, 613, 27011, 294, 3832, 11049, 1889, 1668, 13, 51665], "temperature": 0.0, "avg_logprob": -0.09287765196391515, "compression_ratio": 1.6, "no_speech_prob": 4.7604949031665456e-06}, {"id": 67, "seek": 26500, "start": 291.0, "end": 294.0, "text": " So, zeros, then ones, then twos, and so on.", "tokens": [51665, 407, 11, 35193, 11, 550, 2306, 11, 550, 683, 329, 11, 293, 370, 322, 13, 51815], "temperature": 0.0, "avg_logprob": -0.09287765196391515, "compression_ratio": 1.6, "no_speech_prob": 4.7604949031665456e-06}, {"id": 68, "seek": 29400, "start": 294.0, "end": 299.0, "text": " If you look at a bunch of twos, like the ones in the green rectangle, you can see", "tokens": [50365, 759, 291, 574, 412, 257, 3840, 295, 683, 329, 11, 411, 264, 2306, 294, 264, 3092, 21930, 11, 291, 393, 536, 50615], "temperature": 0.0, "avg_logprob": -0.09739542603492737, "compression_ratio": 2.0070671378091873, "no_speech_prob": 2.1386038497439586e-05}, {"id": 69, "seek": 29400, "start": 299.0, "end": 304.0, "text": " that if you knew they were a handwritten digit, you'd probably guess they were", "tokens": [50615, 300, 498, 291, 2586, 436, 645, 257, 1011, 26859, 14293, 11, 291, 1116, 1391, 2041, 436, 645, 50865], "temperature": 0.0, "avg_logprob": -0.09739542603492737, "compression_ratio": 2.0070671378091873, "no_speech_prob": 2.1386038497439586e-05}, {"id": 70, "seek": 29400, "start": 304.0, "end": 307.0, "text": " twos. But it's very hard to say what it is that", "tokens": [50865, 683, 329, 13, 583, 309, 311, 588, 1152, 281, 584, 437, 309, 307, 300, 51015], "temperature": 0.0, "avg_logprob": -0.09739542603492737, "compression_ratio": 2.0070671378091873, "no_speech_prob": 2.1386038497439586e-05}, {"id": 71, "seek": 29400, "start": 307.0, "end": 310.0, "text": " makes them twos. There's nothing simple that they all have", "tokens": [51015, 1669, 552, 683, 329, 13, 821, 311, 1825, 2199, 300, 436, 439, 362, 51165], "temperature": 0.0, "avg_logprob": -0.09739542603492737, "compression_ratio": 2.0070671378091873, "no_speech_prob": 2.1386038497439586e-05}, {"id": 72, "seek": 29400, "start": 310.0, "end": 313.0, "text": " in common. In particular, if you try and overlay one", "tokens": [51165, 294, 2689, 13, 682, 1729, 11, 498, 291, 853, 293, 31741, 472, 51315], "temperature": 0.0, "avg_logprob": -0.09739542603492737, "compression_ratio": 2.0070671378091873, "no_speech_prob": 2.1386038497439586e-05}, {"id": 73, "seek": 29400, "start": 313.0, "end": 318.0, "text": " on another, you'll see it doesn't fit. And even if you skew it a bit, it's very", "tokens": [51315, 322, 1071, 11, 291, 603, 536, 309, 1177, 380, 3318, 13, 400, 754, 498, 291, 8756, 86, 309, 257, 857, 11, 309, 311, 588, 51565], "temperature": 0.0, "avg_logprob": -0.09739542603492737, "compression_ratio": 2.0070671378091873, "no_speech_prob": 2.1386038497439586e-05}, {"id": 74, "seek": 29400, "start": 318.0, "end": 323.0, "text": " hard to make them overlay on each other. So, a template isn't going to do the job.", "tokens": [51565, 1152, 281, 652, 552, 31741, 322, 1184, 661, 13, 407, 11, 257, 12379, 1943, 380, 516, 281, 360, 264, 1691, 13, 51815], "temperature": 0.0, "avg_logprob": -0.09739542603492737, "compression_ratio": 2.0070671378091873, "no_speech_prob": 2.1386038497439586e-05}, {"id": 75, "seek": 29400, "start": 323.0, "end": 324.0, "text": " And in particular, if you try and overlay one on another, you'll see it doesn't fit.", "tokens": [51815, 400, 294, 1729, 11, 498, 291, 853, 293, 31741, 472, 322, 1071, 11, 291, 603, 536, 309, 1177, 380, 3318, 13, 51865], "temperature": 0.0, "avg_logprob": -0.09739542603492737, "compression_ratio": 2.0070671378091873, "no_speech_prob": 2.1386038497439586e-05}, {"id": 76, "seek": 32400, "start": 324.0, "end": 328.0, "text": " And in particular, a template is going to be very hard to find that'll fit those", "tokens": [50365, 400, 294, 1729, 11, 257, 12379, 307, 516, 281, 312, 588, 1152, 281, 915, 300, 603, 3318, 729, 50565], "temperature": 0.0, "avg_logprob": -0.08620680703057183, "compression_ratio": 1.7269503546099292, "no_speech_prob": 4.9253217184741516e-06}, {"id": 77, "seek": 32400, "start": 328.0, "end": 333.0, "text": " twos in the green box and won't also fit the things in the red boxes.", "tokens": [50565, 683, 329, 294, 264, 3092, 2424, 293, 1582, 380, 611, 3318, 264, 721, 294, 264, 2182, 9002, 13, 50815], "temperature": 0.0, "avg_logprob": -0.08620680703057183, "compression_ratio": 1.7269503546099292, "no_speech_prob": 4.9253217184741516e-06}, {"id": 78, "seek": 32400, "start": 333.0, "end": 338.0, "text": " So, that's one thing that makes recognizing handwritten digits a good task for machine", "tokens": [50815, 407, 11, 300, 311, 472, 551, 300, 1669, 18538, 1011, 26859, 27011, 257, 665, 5633, 337, 3479, 51065], "temperature": 0.0, "avg_logprob": -0.08620680703057183, "compression_ratio": 1.7269503546099292, "no_speech_prob": 4.9253217184741516e-06}, {"id": 79, "seek": 32400, "start": 338.0, "end": 341.0, "text": " learning. Now, I don't want you to think that's the", "tokens": [51065, 2539, 13, 823, 11, 286, 500, 380, 528, 291, 281, 519, 300, 311, 264, 51215], "temperature": 0.0, "avg_logprob": -0.08620680703057183, "compression_ratio": 1.7269503546099292, "no_speech_prob": 4.9253217184741516e-06}, {"id": 80, "seek": 32400, "start": 341.0, "end": 345.0, "text": " only thing we can do. It's a relatively simple thing for a machine", "tokens": [51215, 787, 551, 321, 393, 360, 13, 467, 311, 257, 7226, 2199, 551, 337, 257, 3479, 51415], "temperature": 0.0, "avg_logprob": -0.08620680703057183, "compression_ratio": 1.7269503546099292, "no_speech_prob": 4.9253217184741516e-06}, {"id": 81, "seek": 32400, "start": 345.0, "end": 349.0, "text": " learning system to do now. And to motivate the rest of the course, I", "tokens": [51415, 2539, 1185, 281, 360, 586, 13, 400, 281, 28497, 264, 1472, 295, 264, 1164, 11, 286, 51615], "temperature": 0.0, "avg_logprob": -0.08620680703057183, "compression_ratio": 1.7269503546099292, "no_speech_prob": 4.9253217184741516e-06}, {"id": 82, "seek": 32400, "start": 349.0, "end": 353.0, "text": " want to show you some examples of much more difficult things.", "tokens": [51615, 528, 281, 855, 291, 512, 5110, 295, 709, 544, 2252, 721, 13, 51815], "temperature": 0.0, "avg_logprob": -0.08620680703057183, "compression_ratio": 1.7269503546099292, "no_speech_prob": 4.9253217184741516e-06}, {"id": 83, "seek": 35300, "start": 353.0, "end": 358.0, "text": " So, we now have neural nets with approaching 100 million parameters in", "tokens": [50365, 407, 11, 321, 586, 362, 18161, 36170, 365, 14908, 2319, 2459, 9834, 294, 50615], "temperature": 0.0, "avg_logprob": -0.1391843579850107, "compression_ratio": 1.6377358490566039, "no_speech_prob": 0.00012280639202799648}, {"id": 84, "seek": 35300, "start": 358.0, "end": 365.0, "text": " them that can recognize a thousand different object classes in 1.3 million", "tokens": [50615, 552, 300, 393, 5521, 257, 4714, 819, 2657, 5359, 294, 502, 13, 18, 2459, 50965], "temperature": 0.0, "avg_logprob": -0.1391843579850107, "compression_ratio": 1.6377358490566039, "no_speech_prob": 0.00012280639202799648}, {"id": 85, "seek": 35300, "start": 365.0, "end": 369.0, "text": " high resolution training images got from the web.", "tokens": [50965, 1090, 8669, 3097, 5267, 658, 490, 264, 3670, 13, 51165], "temperature": 0.0, "avg_logprob": -0.1391843579850107, "compression_ratio": 1.6377358490566039, "no_speech_prob": 0.00012280639202799648}, {"id": 86, "seek": 35300, "start": 369.0, "end": 375.0, "text": " So, there was a competition in 2010 and the best system got 47% error rate if you", "tokens": [51165, 407, 11, 456, 390, 257, 6211, 294, 9657, 293, 264, 1151, 1185, 658, 16953, 4, 6713, 3314, 498, 291, 51465], "temperature": 0.0, "avg_logprob": -0.1391843579850107, "compression_ratio": 1.6377358490566039, "no_speech_prob": 0.00012280639202799648}, {"id": 87, "seek": 35300, "start": 375.0, "end": 380.0, "text": " look at its first choice and 25% error rate if you say it got it right if it was", "tokens": [51465, 574, 412, 1080, 700, 3922, 293, 3552, 4, 6713, 3314, 498, 291, 584, 309, 658, 309, 558, 498, 309, 390, 51715], "temperature": 0.0, "avg_logprob": -0.1391843579850107, "compression_ratio": 1.6377358490566039, "no_speech_prob": 0.00012280639202799648}, {"id": 88, "seek": 35300, "start": 380.0, "end": 382.0, "text": " in its top five choices, which isn't bad for a thousand percent error rate.", "tokens": [51715, 294, 1080, 1192, 1732, 7994, 11, 597, 1943, 380, 1578, 337, 257, 4714, 3043, 6713, 3314, 13, 51815], "temperature": 0.0, "avg_logprob": -0.1391843579850107, "compression_ratio": 1.6377358490566039, "no_speech_prob": 0.00012280639202799648}, {"id": 89, "seek": 38200, "start": 382.0, "end": 385.0, "text": " Which isn't bad for a thousand different objects.", "tokens": [50365, 3013, 1943, 380, 1578, 337, 257, 4714, 819, 6565, 13, 50515], "temperature": 0.0, "avg_logprob": -0.10322069688276811, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.00011780275963246822}, {"id": 90, "seek": 38200, "start": 385.0, "end": 391.0, "text": " Jitendra Malik, who's an eminent neural net skeptic and a leading computer vision", "tokens": [50515, 508, 270, 27332, 5746, 1035, 11, 567, 311, 364, 846, 11058, 18161, 2533, 19128, 299, 293, 257, 5775, 3820, 5201, 50815], "temperature": 0.0, "avg_logprob": -0.10322069688276811, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.00011780275963246822}, {"id": 91, "seek": 38200, "start": 391.0, "end": 396.0, "text": " researcher has said that this competition is a good test of whether deep neural", "tokens": [50815, 21751, 575, 848, 300, 341, 6211, 307, 257, 665, 1500, 295, 1968, 2452, 18161, 51065], "temperature": 0.0, "avg_logprob": -0.10322069688276811, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.00011780275963246822}, {"id": 92, "seek": 38200, "start": 396.0, "end": 403.0, "text": " networks can work well for object recognition. And a very deep neural network can", "tokens": [51065, 9590, 393, 589, 731, 337, 2657, 11150, 13, 400, 257, 588, 2452, 18161, 3209, 393, 51415], "temperature": 0.0, "avg_logprob": -0.10322069688276811, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.00011780275963246822}, {"id": 93, "seek": 38200, "start": 403.0, "end": 407.0, "text": " now do considerably better than the thing that won the competition.", "tokens": [51415, 586, 360, 31308, 1101, 813, 264, 551, 300, 1582, 264, 6211, 13, 51615], "temperature": 0.0, "avg_logprob": -0.10322069688276811, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.00011780275963246822}, {"id": 94, "seek": 38200, "start": 407.0, "end": 411.0, "text": " It can get less than 40% error for its first choice and less than 20% error for", "tokens": [51615, 467, 393, 483, 1570, 813, 3356, 4, 6713, 337, 1080, 700, 3922, 293, 1570, 813, 945, 4, 6713, 337, 51815], "temperature": 0.0, "avg_logprob": -0.10322069688276811, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.00011780275963246822}, {"id": 95, "seek": 38200, "start": 411.0, "end": 412.0, "text": " its top five choices.", "tokens": [51815, 1080, 1192, 1732, 7994, 13, 51865], "temperature": 0.0, "avg_logprob": -0.10322069688276811, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.00011780275963246822}, {"id": 96, "seek": 41200, "start": 412.0, "end": 415.0, "text": " I'll describe that in much more detail in lecture five.", "tokens": [50365, 286, 603, 6786, 300, 294, 709, 544, 2607, 294, 7991, 1732, 13, 50515], "temperature": 0.0, "avg_logprob": -0.13566226050967262, "compression_ratio": 1.6958041958041958, "no_speech_prob": 2.3336548110819422e-05}, {"id": 97, "seek": 41200, "start": 415.0, "end": 419.0, "text": " Here's some examples of the kinds of images you have to recognize.", "tokens": [50515, 1692, 311, 512, 5110, 295, 264, 3685, 295, 5267, 291, 362, 281, 5521, 13, 50715], "temperature": 0.0, "avg_logprob": -0.13566226050967262, "compression_ratio": 1.6958041958041958, "no_speech_prob": 2.3336548110819422e-05}, {"id": 98, "seek": 41200, "start": 419.0, "end": 423.0, "text": " These are images from the test set that it's never seen before.", "tokens": [50715, 1981, 366, 5267, 490, 264, 1500, 992, 300, 309, 311, 1128, 1612, 949, 13, 50915], "temperature": 0.0, "avg_logprob": -0.13566226050967262, "compression_ratio": 1.6958041958041958, "no_speech_prob": 2.3336548110819422e-05}, {"id": 99, "seek": 41200, "start": 423.0, "end": 428.0, "text": " And below the examples, I'm showing you what the neural net thought the right", "tokens": [50915, 400, 2507, 264, 5110, 11, 286, 478, 4099, 291, 437, 264, 18161, 2533, 1194, 264, 558, 51165], "temperature": 0.0, "avg_logprob": -0.13566226050967262, "compression_ratio": 1.6958041958041958, "no_speech_prob": 2.3336548110819422e-05}, {"id": 100, "seek": 41200, "start": 428.0, "end": 433.0, "text": " answer was, where the length of the horizontal bar is how confident it was,", "tokens": [51165, 1867, 390, 11, 689, 264, 4641, 295, 264, 12750, 2159, 307, 577, 6679, 309, 390, 11, 51415], "temperature": 0.0, "avg_logprob": -0.13566226050967262, "compression_ratio": 1.6958041958041958, "no_speech_prob": 2.3336548110819422e-05}, {"id": 101, "seek": 41200, "start": 433.0, "end": 438.0, "text": " and the correct answer is in red. So, if you look in the middle, it correctly", "tokens": [51415, 293, 264, 3006, 1867, 307, 294, 2182, 13, 407, 11, 498, 291, 574, 294, 264, 2808, 11, 309, 8944, 51665], "temperature": 0.0, "avg_logprob": -0.13566226050967262, "compression_ratio": 1.6958041958041958, "no_speech_prob": 2.3336548110819422e-05}, {"id": 102, "seek": 41200, "start": 438.0, "end": 441.0, "text": " identified that as a snowplow. But you can see that it's unbiased.", "tokens": [51665, 9234, 300, 382, 257, 5756, 564, 305, 13, 583, 291, 393, 536, 300, 309, 311, 517, 5614, 1937, 13, 51815], "temperature": 0.0, "avg_logprob": -0.13566226050967262, "compression_ratio": 1.6958041958041958, "no_speech_prob": 2.3336548110819422e-05}, {"id": 103, "seek": 44100, "start": 441.0, "end": 444.0, "text": " But you can see that its other choices were also fairly sensible.", "tokens": [50365, 583, 291, 393, 536, 300, 1080, 661, 7994, 645, 611, 6457, 25380, 13, 50515], "temperature": 0.0, "avg_logprob": -0.12817073020206135, "compression_ratio": 1.9834983498349834, "no_speech_prob": 5.161984881851822e-05}, {"id": 104, "seek": 44100, "start": 444.0, "end": 447.0, "text": " It does look a little bit like a drilling platform.", "tokens": [50515, 467, 775, 574, 257, 707, 857, 411, 257, 26290, 3663, 13, 50665], "temperature": 0.0, "avg_logprob": -0.12817073020206135, "compression_ratio": 1.9834983498349834, "no_speech_prob": 5.161984881851822e-05}, {"id": 105, "seek": 44100, "start": 447.0, "end": 450.0, "text": " And if you look at its third choice, a lifeboat, it actually looks very like a", "tokens": [50665, 400, 498, 291, 574, 412, 1080, 2636, 3922, 11, 257, 993, 31883, 11, 309, 767, 1542, 588, 411, 257, 50815], "temperature": 0.0, "avg_logprob": -0.12817073020206135, "compression_ratio": 1.9834983498349834, "no_speech_prob": 5.161984881851822e-05}, {"id": 106, "seek": 44100, "start": 450.0, "end": 452.0, "text": " lifeboat. You can see the flag on the front of the", "tokens": [50815, 993, 31883, 13, 509, 393, 536, 264, 7166, 322, 264, 1868, 295, 264, 50915], "temperature": 0.0, "avg_logprob": -0.12817073020206135, "compression_ratio": 1.9834983498349834, "no_speech_prob": 5.161984881851822e-05}, {"id": 107, "seek": 44100, "start": 452.0, "end": 456.0, "text": " boat and the bridge of the boat and the flag at the back and the high surf in the", "tokens": [50915, 6582, 293, 264, 7283, 295, 264, 6582, 293, 264, 7166, 412, 264, 646, 293, 264, 1090, 9684, 294, 264, 51115], "temperature": 0.0, "avg_logprob": -0.12817073020206135, "compression_ratio": 1.9834983498349834, "no_speech_prob": 5.161984881851822e-05}, {"id": 108, "seek": 44100, "start": 456.0, "end": 459.0, "text": " background. So it's, its errors tell you a lot about", "tokens": [51115, 3678, 13, 407, 309, 311, 11, 1080, 13603, 980, 291, 257, 688, 466, 51265], "temperature": 0.0, "avg_logprob": -0.12817073020206135, "compression_ratio": 1.9834983498349834, "no_speech_prob": 5.161984881851822e-05}, {"id": 109, "seek": 44100, "start": 459.0, "end": 462.0, "text": " how it's doing it and they're very plausible errors.", "tokens": [51265, 577, 309, 311, 884, 309, 293, 436, 434, 588, 39925, 13603, 13, 51415], "temperature": 0.0, "avg_logprob": -0.12817073020206135, "compression_ratio": 1.9834983498349834, "no_speech_prob": 5.161984881851822e-05}, {"id": 110, "seek": 44100, "start": 462.0, "end": 467.0, "text": " If you look on the left, it gets it wrong, possibly because the beak of the bird is", "tokens": [51415, 759, 291, 574, 322, 264, 1411, 11, 309, 2170, 309, 2085, 11, 6264, 570, 264, 48663, 295, 264, 5255, 307, 51665], "temperature": 0.0, "avg_logprob": -0.12817073020206135, "compression_ratio": 1.9834983498349834, "no_speech_prob": 5.161984881851822e-05}, {"id": 111, "seek": 44100, "start": 467.0, "end": 470.0, "text": " missing and because the feathers of the bird look very like the flag of the bird.", "tokens": [51665, 5361, 293, 570, 264, 27044, 295, 264, 5255, 574, 588, 411, 264, 7166, 295, 264, 5255, 13, 51815], "temperature": 0.0, "avg_logprob": -0.12817073020206135, "compression_ratio": 1.9834983498349834, "no_speech_prob": 5.161984881851822e-05}, {"id": 112, "seek": 47000, "start": 470.0, "end": 475.0, "text": " Look very like the wet fur of an otter. But it gets it in its top five and it", "tokens": [50365, 2053, 588, 411, 264, 6630, 2687, 295, 364, 4337, 391, 13, 583, 309, 2170, 309, 294, 1080, 1192, 1732, 293, 309, 50615], "temperature": 0.0, "avg_logprob": -0.09520312323086504, "compression_ratio": 1.7006802721088434, "no_speech_prob": 3.901518357452005e-05}, {"id": 113, "seek": 47000, "start": 475.0, "end": 478.0, "text": " does better than me. I wouldn't know if that was a quail or", "tokens": [50615, 775, 1101, 813, 385, 13, 286, 2759, 380, 458, 498, 300, 390, 257, 421, 864, 420, 50765], "temperature": 0.0, "avg_logprob": -0.09520312323086504, "compression_ratio": 1.7006802721088434, "no_speech_prob": 3.901518357452005e-05}, {"id": 114, "seek": 47000, "start": 478.0, "end": 482.0, "text": " a roughed grouse or a partridge. If you look on the right, it gets it", "tokens": [50765, 257, 5903, 292, 677, 1316, 420, 257, 644, 15804, 13, 759, 291, 574, 322, 264, 558, 11, 309, 2170, 309, 50965], "temperature": 0.0, "avg_logprob": -0.09520312323086504, "compression_ratio": 1.7006802721088434, "no_speech_prob": 3.901518357452005e-05}, {"id": 115, "seek": 47000, "start": 482.0, "end": 486.0, "text": " completely wrong. It a guillotine, you can see why it says", "tokens": [50965, 2584, 2085, 13, 467, 257, 695, 373, 39658, 11, 291, 393, 536, 983, 309, 1619, 51165], "temperature": 0.0, "avg_logprob": -0.09520312323086504, "compression_ratio": 1.7006802721088434, "no_speech_prob": 3.901518357452005e-05}, {"id": 116, "seek": 47000, "start": 486.0, "end": 489.0, "text": " that. You can possibly see why it says orangutan", "tokens": [51165, 300, 13, 509, 393, 6264, 536, 983, 309, 1619, 17481, 30624, 51315], "temperature": 0.0, "avg_logprob": -0.09520312323086504, "compression_ratio": 1.7006802721088434, "no_speech_prob": 3.901518357452005e-05}, {"id": 117, "seek": 47000, "start": 489.0, "end": 492.0, "text": " because of the sort of jungle looking background and something orange in the", "tokens": [51315, 570, 295, 264, 1333, 295, 18228, 1237, 3678, 293, 746, 7671, 294, 264, 51465], "temperature": 0.0, "avg_logprob": -0.09520312323086504, "compression_ratio": 1.7006802721088434, "no_speech_prob": 3.901518357452005e-05}, {"id": 118, "seek": 47000, "start": 492.0, "end": 495.0, "text": " middle. But it fails to get the right answer.", "tokens": [51465, 2808, 13, 583, 309, 18199, 281, 483, 264, 558, 1867, 13, 51615], "temperature": 0.0, "avg_logprob": -0.09520312323086504, "compression_ratio": 1.7006802721088434, "no_speech_prob": 3.901518357452005e-05}, {"id": 119, "seek": 47000, "start": 495.0, "end": 499.0, "text": " It can, however, deal with a wide range of different objects.", "tokens": [51615, 467, 393, 11, 4461, 11, 2028, 365, 257, 4874, 3613, 295, 819, 6565, 13, 51815], "temperature": 0.0, "avg_logprob": -0.09520312323086504, "compression_ratio": 1.7006802721088434, "no_speech_prob": 3.901518357452005e-05}, {"id": 120, "seek": 50000, "start": 500.0, "end": 505.0, "text": " If you look on the left, I would have said microwave was my first answer.", "tokens": [50365, 759, 291, 574, 322, 264, 1411, 11, 286, 576, 362, 848, 19025, 390, 452, 700, 1867, 13, 50615], "temperature": 0.0, "avg_logprob": -0.12433412506824404, "compression_ratio": 1.65993265993266, "no_speech_prob": 8.694550342625007e-05}, {"id": 121, "seek": 50000, "start": 505.0, "end": 509.0, "text": " The labels aren't very systematic, so actually the correct answer there is", "tokens": [50615, 440, 16949, 3212, 380, 588, 27249, 11, 370, 767, 264, 3006, 1867, 456, 307, 50815], "temperature": 0.0, "avg_logprob": -0.12433412506824404, "compression_ratio": 1.65993265993266, "no_speech_prob": 8.694550342625007e-05}, {"id": 122, "seek": 50000, "start": 509.0, "end": 513.0, "text": " electric range and does get it in its top five.", "tokens": [50815, 5210, 3613, 293, 775, 483, 309, 294, 1080, 1192, 1732, 13, 51015], "temperature": 0.0, "avg_logprob": -0.12433412506824404, "compression_ratio": 1.65993265993266, "no_speech_prob": 8.694550342625007e-05}, {"id": 123, "seek": 50000, "start": 513.0, "end": 516.0, "text": " In the middle, it's getting a turnstile, which is a distributed object.", "tokens": [51015, 682, 264, 2808, 11, 309, 311, 1242, 257, 1261, 372, 794, 11, 597, 307, 257, 12631, 2657, 13, 51165], "temperature": 0.0, "avg_logprob": -0.12433412506824404, "compression_ratio": 1.65993265993266, "no_speech_prob": 8.694550342625007e-05}, {"id": 124, "seek": 50000, "start": 516.0, "end": 520.0, "text": " It does, it can do more than just recognize compact things.", "tokens": [51165, 467, 775, 11, 309, 393, 360, 544, 813, 445, 5521, 14679, 721, 13, 51365], "temperature": 0.0, "avg_logprob": -0.12433412506824404, "compression_ratio": 1.65993265993266, "no_speech_prob": 8.694550342625007e-05}, {"id": 125, "seek": 50000, "start": 520.0, "end": 525.0, "text": " And it can also deal with pictures as well as real scenes, like the bulletproof vest.", "tokens": [51365, 400, 309, 393, 611, 2028, 365, 5242, 382, 731, 382, 957, 8026, 11, 411, 264, 11632, 15690, 15814, 13, 51615], "temperature": 0.0, "avg_logprob": -0.12433412506824404, "compression_ratio": 1.65993265993266, "no_speech_prob": 8.694550342625007e-05}, {"id": 126, "seek": 50000, "start": 525.0, "end": 530.0, "text": " And it makes some very cool errors. If you look at the image on the left, it's", "tokens": [51615, 400, 309, 1669, 512, 588, 1627, 13603, 13, 759, 291, 574, 412, 264, 3256, 322, 264, 1411, 11, 309, 311, 51865], "temperature": 0.0, "avg_logprob": -0.12433412506824404, "compression_ratio": 1.65993265993266, "no_speech_prob": 8.694550342625007e-05}, {"id": 127, "seek": 53000, "start": 530.0, "end": 534.0, "text": " an earphone. It doesn't get anything like an earphone.", "tokens": [50365, 364, 1273, 4977, 13, 467, 1177, 380, 483, 1340, 411, 364, 1273, 4977, 13, 50565], "temperature": 0.0, "avg_logprob": -0.16060561890814715, "compression_ratio": 1.8791946308724832, "no_speech_prob": 6.344221765175462e-05}, {"id": 128, "seek": 53000, "start": 534.0, "end": 537.0, "text": " But if you look at its fourth bet, it thinks it's an ant.", "tokens": [50565, 583, 498, 291, 574, 412, 1080, 6409, 778, 11, 309, 7309, 309, 311, 364, 2511, 13, 50715], "temperature": 0.0, "avg_logprob": -0.16060561890814715, "compression_ratio": 1.8791946308724832, "no_speech_prob": 6.344221765175462e-05}, {"id": 129, "seek": 53000, "start": 537.0, "end": 541.0, "text": " And to begin with, you think that's crazy. But then if you look at it carefully, you", "tokens": [50715, 400, 281, 1841, 365, 11, 291, 519, 300, 311, 3219, 13, 583, 550, 498, 291, 574, 412, 309, 7500, 11, 291, 50915], "temperature": 0.0, "avg_logprob": -0.16060561890814715, "compression_ratio": 1.8791946308724832, "no_speech_prob": 6.344221765175462e-05}, {"id": 130, "seek": 53000, "start": 541.0, "end": 544.0, "text": " can see it's a view from an ant from underneath. The eyes are looking down at", "tokens": [50915, 393, 536, 309, 311, 257, 1910, 490, 364, 2511, 490, 7223, 13, 440, 2575, 366, 1237, 760, 412, 51065], "temperature": 0.0, "avg_logprob": -0.16060561890814715, "compression_ratio": 1.8791946308724832, "no_speech_prob": 6.344221765175462e-05}, {"id": 131, "seek": 53000, "start": 544.0, "end": 548.0, "text": " you, and you can see the antennae behind it. It's not the kind of view of an ant", "tokens": [51065, 291, 11, 293, 291, 393, 536, 264, 24573, 68, 2261, 309, 13, 467, 311, 406, 264, 733, 295, 1910, 295, 364, 2511, 51265], "temperature": 0.0, "avg_logprob": -0.16060561890814715, "compression_ratio": 1.8791946308724832, "no_speech_prob": 6.344221765175462e-05}, {"id": 132, "seek": 53000, "start": 548.0, "end": 552.0, "text": " you'd like to have if you were a green fly. If you look at the one on the right,", "tokens": [51265, 291, 1116, 411, 281, 362, 498, 291, 645, 257, 3092, 3603, 13, 759, 291, 574, 412, 264, 472, 322, 264, 558, 11, 51465], "temperature": 0.0, "avg_logprob": -0.16060561890814715, "compression_ratio": 1.8791946308724832, "no_speech_prob": 6.344221765175462e-05}, {"id": 133, "seek": 53000, "start": 552.0, "end": 556.0, "text": " it doesn't get the right answer. But all of its answers are cylindrical", "tokens": [51465, 309, 1177, 380, 483, 264, 558, 1867, 13, 583, 439, 295, 1080, 6338, 366, 28044, 15888, 51665], "temperature": 0.0, "avg_logprob": -0.16060561890814715, "compression_ratio": 1.8791946308724832, "no_speech_prob": 6.344221765175462e-05}, {"id": 134, "seek": 53000, "start": 556.0, "end": 560.0, "text": " objects. Another question that I've been asked is,", "tokens": [51665, 6565, 13, 3996, 1168, 300, 286, 600, 668, 2351, 307, 11, 51865], "temperature": 0.0, "avg_logprob": -0.16060561890814715, "compression_ratio": 1.8791946308724832, "no_speech_prob": 6.344221765175462e-05}, {"id": 135, "seek": 56000, "start": 560.0, "end": 564.0, "text": " what is the difference between a sound wave and a speech recognition system?", "tokens": [50365, 437, 307, 264, 2649, 1296, 257, 1626, 5772, 293, 257, 6218, 11150, 1185, 30, 50565], "temperature": 0.0, "avg_logprob": -0.2631339856556484, "compression_ratio": 1.913533834586466, "no_speech_prob": 4.909524795948528e-05}, {"id": 136, "seek": 56000, "start": 564.0, "end": 568.0, "text": " And another task that neural nets are now very good at is speech recognition.", "tokens": [50565, 400, 1071, 5633, 300, 18161, 36170, 366, 586, 588, 665, 412, 307, 6218, 11150, 13, 50765], "temperature": 0.0, "avg_logprob": -0.2631339856556484, "compression_ratio": 1.913533834586466, "no_speech_prob": 4.909524795948528e-05}, {"id": 137, "seek": 56000, "start": 568.0, "end": 572.0, "text": " Or at least part of a speech recognition system. So speech recognition systems have", "tokens": [50765, 1610, 412, 1935, 644, 295, 257, 6218, 11150, 1185, 13, 407, 6218, 11150, 3652, 362, 50965], "temperature": 0.0, "avg_logprob": -0.2631339856556484, "compression_ratio": 1.913533834586466, "no_speech_prob": 4.909524795948528e-05}, {"id": 138, "seek": 56000, "start": 572.0, "end": 576.0, "text": " several stages. First they pre-process the sound wave to", "tokens": [50965, 2940, 10232, 13, 2386, 436, 659, 12, 41075, 264, 1626, 5772, 281, 51165], "temperature": 0.0, "avg_logprob": -0.2631339856556484, "compression_ratio": 1.913533834586466, "no_speech_prob": 4.909524795948528e-05}, {"id": 139, "seek": 56000, "start": 576.0, "end": 581.0, "text": " get a vector of acoustic coefficients for each 10 milliseconds of sound wave.", "tokens": [51165, 483, 257, 8062, 295, 26753, 31994, 337, 1184, 1266, 34184, 295, 1626, 5772, 13, 51415], "temperature": 0.0, "avg_logprob": -0.2631339856556484, "compression_ratio": 1.913533834586466, "no_speech_prob": 4.909524795948528e-05}, {"id": 140, "seek": 56000, "start": 581.0, "end": 585.0, "text": " And so they get a hundred of those vectors per second.", "tokens": [51415, 400, 370, 436, 483, 257, 3262, 295, 729, 18875, 680, 1150, 13, 51615], "temperature": 0.0, "avg_logprob": -0.2631339856556484, "compression_ratio": 1.913533834586466, "no_speech_prob": 4.909524795948528e-05}, {"id": 141, "seek": 56000, "start": 585.0, "end": 590.0, "text": " They then take a few adjacent vectors of acoustic coefficients, and they need to", "tokens": [51615, 814, 550, 747, 257, 1326, 24441, 18875, 295, 26753, 31994, 11, 293, 436, 643, 281, 51865], "temperature": 0.0, "avg_logprob": -0.2631339856556484, "compression_ratio": 1.913533834586466, "no_speech_prob": 4.909524795948528e-05}, {"id": 142, "seek": 59000, "start": 590.0, "end": 595.0, "text": " place bets on which part of which phoneme is being spoken. So they look at this", "tokens": [50365, 1081, 39922, 322, 597, 644, 295, 597, 30754, 5729, 307, 885, 10759, 13, 407, 436, 574, 412, 341, 50615], "temperature": 0.0, "avg_logprob": -0.07694985071818033, "compression_ratio": 1.9761904761904763, "no_speech_prob": 0.00013536626647692174}, {"id": 143, "seek": 59000, "start": 595.0, "end": 599.0, "text": " little window, and they say in the middle of this window, what do I think the", "tokens": [50615, 707, 4910, 11, 293, 436, 584, 294, 264, 2808, 295, 341, 4910, 11, 437, 360, 286, 519, 264, 50815], "temperature": 0.0, "avg_logprob": -0.07694985071818033, "compression_ratio": 1.9761904761904763, "no_speech_prob": 0.00013536626647692174}, {"id": 144, "seek": 59000, "start": 599.0, "end": 604.0, "text": " phoneme is, and which part of the phoneme is it? And a good speech recognition", "tokens": [50815, 30754, 5729, 307, 11, 293, 597, 644, 295, 264, 30754, 5729, 307, 309, 30, 400, 257, 665, 6218, 11150, 51065], "temperature": 0.0, "avg_logprob": -0.07694985071818033, "compression_ratio": 1.9761904761904763, "no_speech_prob": 0.00013536626647692174}, {"id": 145, "seek": 59000, "start": 604.0, "end": 609.0, "text": " system will have many alternative models for a phoneme. And each model it might", "tokens": [51065, 1185, 486, 362, 867, 8535, 5245, 337, 257, 30754, 5729, 13, 400, 1184, 2316, 309, 1062, 51315], "temperature": 0.0, "avg_logprob": -0.07694985071818033, "compression_ratio": 1.9761904761904763, "no_speech_prob": 0.00013536626647692174}, {"id": 146, "seek": 59000, "start": 609.0, "end": 614.0, "text": " have three different parts. So it might have many thousands of alternative", "tokens": [51315, 362, 1045, 819, 3166, 13, 407, 309, 1062, 362, 867, 5383, 295, 8535, 51565], "temperature": 0.0, "avg_logprob": -0.07694985071818033, "compression_ratio": 1.9761904761904763, "no_speech_prob": 0.00013536626647692174}, {"id": 147, "seek": 59000, "start": 614.0, "end": 618.0, "text": " fragments that it thinks this might be. And you have to place bets on all those", "tokens": [51565, 29197, 300, 309, 7309, 341, 1062, 312, 13, 400, 291, 362, 281, 1081, 39922, 322, 439, 729, 51765], "temperature": 0.0, "avg_logprob": -0.07694985071818033, "compression_ratio": 1.9761904761904763, "no_speech_prob": 0.00013536626647692174}, {"id": 148, "seek": 59000, "start": 618.0, "end": 619.0, "text": " thousands of alternatives.", "tokens": [51765, 5383, 295, 20478, 13, 51815], "temperature": 0.0, "avg_logprob": -0.07694985071818033, "compression_ratio": 1.9761904761904763, "no_speech_prob": 0.00013536626647692174}, {"id": 149, "seek": 62000, "start": 620.0, "end": 625.0, "text": " And then once you place those bets, you have a decoding stage that does the best", "tokens": [50365, 400, 550, 1564, 291, 1081, 729, 39922, 11, 291, 362, 257, 979, 8616, 3233, 300, 775, 264, 1151, 50615], "temperature": 0.0, "avg_logprob": -0.12615873416264853, "compression_ratio": 1.5687022900763359, "no_speech_prob": 8.083567081484944e-05}, {"id": 150, "seek": 62000, "start": 625.0, "end": 632.0, "text": " job it can of using plausible bets, but piecing them together into a sequence of", "tokens": [50615, 1691, 309, 393, 295, 1228, 39925, 39922, 11, 457, 1730, 2175, 552, 1214, 666, 257, 8310, 295, 50965], "temperature": 0.0, "avg_logprob": -0.12615873416264853, "compression_ratio": 1.5687022900763359, "no_speech_prob": 8.083567081484944e-05}, {"id": 151, "seek": 62000, "start": 632.0, "end": 637.0, "text": " bets that corresponds to the kinds of things that people say.", "tokens": [50965, 39922, 300, 23249, 281, 264, 3685, 295, 721, 300, 561, 584, 13, 51215], "temperature": 0.0, "avg_logprob": -0.12615873416264853, "compression_ratio": 1.5687022900763359, "no_speech_prob": 8.083567081484944e-05}, {"id": 152, "seek": 62000, "start": 637.0, "end": 644.0, "text": " Currently, deep neural networks pioneered by George Dahl and Abdul Rahman,", "tokens": [51215, 19964, 11, 2452, 18161, 9590, 19761, 4073, 538, 7136, 413, 10722, 293, 42591, 17844, 1601, 11, 51565], "temperature": 0.0, "avg_logprob": -0.12615873416264853, "compression_ratio": 1.5687022900763359, "no_speech_prob": 8.083567081484944e-05}, {"id": 153, "seek": 62000, "start": 644.0, "end": 648.0, "text": " Mohammed, at the University of Toronto, are doing better than previous machine", "tokens": [51565, 41910, 11, 412, 264, 3535, 295, 14140, 11, 366, 884, 1101, 813, 3894, 3479, 51765], "temperature": 0.0, "avg_logprob": -0.12615873416264853, "compression_ratio": 1.5687022900763359, "no_speech_prob": 8.083567081484944e-05}, {"id": 154, "seek": 62000, "start": 648.0, "end": 650.0, "text": " learning methods for the economy.", "tokens": [51765, 2539, 7150, 337, 264, 5010, 13, 51865], "temperature": 0.0, "avg_logprob": -0.12615873416264853, "compression_ratio": 1.5687022900763359, "no_speech_prob": 8.083567081484944e-05}, {"id": 155, "seek": 65000, "start": 650.0, "end": 653.0, "text": " They're now using the acoustic model, and they're now beginning to be used in", "tokens": [50365, 814, 434, 586, 1228, 264, 26753, 2316, 11, 293, 436, 434, 586, 2863, 281, 312, 1143, 294, 50515], "temperature": 0.0, "avg_logprob": -0.20139410556891027, "compression_ratio": 1.5645933014354068, "no_speech_prob": 6.627895345445722e-05}, {"id": 156, "seek": 65000, "start": 653.0, "end": 664.0, "text": " practical systems. So Dahl and Mohammed developed a system that uses many layers", "tokens": [50515, 8496, 3652, 13, 407, 413, 10722, 293, 41910, 4743, 257, 1185, 300, 4960, 867, 7914, 51065], "temperature": 0.0, "avg_logprob": -0.20139410556891027, "compression_ratio": 1.5645933014354068, "no_speech_prob": 6.627895345445722e-05}, {"id": 157, "seek": 65000, "start": 664.0, "end": 674.0, "text": " of binary neurons to take some acoustic frames and make bets about the labels.", "tokens": [51065, 295, 17434, 22027, 281, 747, 512, 26753, 12083, 293, 652, 39922, 466, 264, 16949, 13, 51565], "temperature": 0.0, "avg_logprob": -0.20139410556891027, "compression_ratio": 1.5645933014354068, "no_speech_prob": 6.627895345445722e-05}, {"id": 158, "seek": 65000, "start": 674.0, "end": 678.0, "text": " They were doing it on a fairly small database, and they only used 183 alternative", "tokens": [51565, 814, 645, 884, 309, 322, 257, 6457, 1359, 8149, 11, 293, 436, 787, 1143, 2443, 18, 8535, 51765], "temperature": 0.0, "avg_logprob": -0.20139410556891027, "compression_ratio": 1.5645933014354068, "no_speech_prob": 6.627895345445722e-05}, {"id": 159, "seek": 65000, "start": 678.0, "end": 679.0, "text": " labels.", "tokens": [51765, 16949, 13, 51815], "temperature": 0.0, "avg_logprob": -0.20139410556891027, "compression_ratio": 1.5645933014354068, "no_speech_prob": 6.627895345445722e-05}, {"id": 160, "seek": 67900, "start": 679.0, "end": 682.0, "text": " And to get their system to work well, they did some pre-training, which will be", "tokens": [50365, 400, 281, 483, 641, 1185, 281, 589, 731, 11, 436, 630, 512, 659, 12, 17227, 1760, 11, 597, 486, 312, 50515], "temperature": 0.0, "avg_logprob": -0.10303943245499222, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.00017230710363946855}, {"id": 161, "seek": 67900, "start": 682.0, "end": 685.0, "text": " described in the second half of the course.", "tokens": [50515, 7619, 294, 264, 1150, 1922, 295, 264, 1164, 13, 50665], "temperature": 0.0, "avg_logprob": -0.10303943245499222, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.00017230710363946855}, {"id": 162, "seek": 67900, "start": 685.0, "end": 691.0, "text": " After standard post-processing, they got 20.7% error rate on a very standard", "tokens": [50665, 2381, 3832, 2183, 12, 41075, 278, 11, 436, 658, 945, 13, 22, 4, 6713, 3314, 322, 257, 588, 3832, 50965], "temperature": 0.0, "avg_logprob": -0.10303943245499222, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.00017230710363946855}, {"id": 163, "seek": 67900, "start": 691.0, "end": 695.0, "text": " benchmark, which is kind of like the MNIST for speech.", "tokens": [50965, 18927, 11, 597, 307, 733, 295, 411, 264, 376, 45, 19756, 337, 6218, 13, 51165], "temperature": 0.0, "avg_logprob": -0.10303943245499222, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.00017230710363946855}, {"id": 164, "seek": 67900, "start": 695.0, "end": 700.0, "text": " The best previous result on that benchmark for speaker independent recognition was", "tokens": [51165, 440, 1151, 3894, 1874, 322, 300, 18927, 337, 8145, 6695, 11150, 390, 51415], "temperature": 0.0, "avg_logprob": -0.10303943245499222, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.00017230710363946855}, {"id": 165, "seek": 67900, "start": 700.0, "end": 707.0, "text": " 24.4%. And a very experienced speech researcher at Microsoft Research realized that that was a big enough", "tokens": [51415, 4022, 13, 19, 6856, 400, 257, 588, 6751, 6218, 21751, 412, 8116, 10303, 5334, 300, 300, 390, 257, 955, 1547, 51765], "temperature": 0.0, "avg_logprob": -0.10303943245499222, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.00017230710363946855}, {"id": 166, "seek": 70700, "start": 707.0, "end": 715.0, "text": " improvement that probably this would change the way speech recognition systems were done, and indeed it has.", "tokens": [50365, 10444, 300, 1391, 341, 576, 1319, 264, 636, 6218, 11150, 3652, 645, 1096, 11, 293, 6451, 309, 575, 13, 50765], "temperature": 0.4, "avg_logprob": -0.40355643247946715, "compression_ratio": 1.4934497816593886, "no_speech_prob": 0.00019204823183827102}, {"id": 167, "seek": 70700, "start": 715.0, "end": 726.0, "text": " So if you look at recent results from several different leading speech groups, Microsoft showed that this kind of deep", "tokens": [50765, 407, 498, 291, 574, 412, 5162, 3542, 490, 2940, 819, 5775, 6218, 3935, 11, 8116, 4712, 300, 341, 733, 295, 2452, 51315], "temperature": 0.4, "avg_logprob": -0.40355643247946715, "compression_ratio": 1.4934497816593886, "no_speech_prob": 0.00019204823183827102}, {"id": 168, "seek": 70700, "start": 726.0, "end": 735.0, "text": " neural network, when used as the acoustic model in a speech system, reduced the error rate from 27.4% to 18.5%,Hi,", "tokens": [51315, 18161, 3209, 11, 562, 1143, 382, 264, 26753, 2316, 294, 257, 6218, 1185, 11, 9212, 264, 6713, 3314, 490, 7634, 13, 19, 4, 281, 2443, 13, 20, 8923, 17155, 11, 51765], "temperature": 0.4, "avg_logprob": -0.40355643247946715, "compression_ratio": 1.4934497816593886, "no_speech_prob": 0.00019204823183827102}, {"id": 169, "seek": 73500, "start": 735.0, "end": 743.0, "text": " and relatively, you could view it as reducing the amount of training data you needed from 2,000 hours down to 309 hours to get comparable performance.", "tokens": [50365, 293, 7226, 11, 291, 727, 1910, 309, 382, 12245, 264, 2372, 295, 3097, 1412, 291, 2978, 490, 568, 11, 1360, 2496, 760, 281, 2217, 24, 2496, 281, 483, 25323, 3389, 13, 50765], "temperature": 0.0, "avg_logprob": -0.2237783974974695, "compression_ratio": 2.127586206896552, "no_speech_prob": 0.00027123047038912773}, {"id": 170, "seek": 73500, "start": 743.0, "end": 761.0, "text": " IBM, which has the best system for one of the standard speech recognition tasks for large vocabulary speech recognition, showed that even its very highly tuned system that was getting 18.8% can be beaten by one of these deep neural networks.", "tokens": [50765, 23487, 11, 597, 575, 264, 1151, 1185, 337, 472, 295, 264, 3832, 6218, 11150, 9608, 337, 2416, 19864, 6218, 11150, 11, 4712, 300, 754, 1080, 588, 5405, 10870, 1185, 300, 390, 1242, 2443, 13, 23, 4, 393, 312, 17909, 538, 472, 295, 613, 2452, 18161, 9590, 13, 51665], "temperature": 0.0, "avg_logprob": -0.2237783974974695, "compression_ratio": 2.127586206896552, "no_speech_prob": 0.00027123047038912773}, {"id": 171, "seek": 73500, "start": 761.0, "end": 763.0, "text": " And Google, fairly recently, trained a deep neural network to do the same.", "tokens": [51665, 400, 3329, 11, 6457, 3938, 11, 8895, 257, 2452, 18161, 3209, 281, 360, 264, 912, 13, 51765], "temperature": 0.0, "avg_logprob": -0.2237783974974695, "compression_ratio": 2.127586206896552, "no_speech_prob": 0.00027123047038912773}, {"id": 172, "seek": 73500, "start": 763.0, "end": 764.0, "text": " And Google, fairly recently, trained a deep neural network to do the same.", "tokens": [51765, 400, 3329, 11, 6457, 3938, 11, 8895, 257, 2452, 18161, 3209, 281, 360, 264, 912, 13, 51815], "temperature": 0.0, "avg_logprob": -0.2237783974974695, "compression_ratio": 2.127586206896552, "no_speech_prob": 0.00027123047038912773}, {"id": 173, "seek": 73500, "start": 764.0, "end": 765.0, "text": " And Google, fairly recently, trained a deep neural network to do the same.", "tokens": [51815, 400, 3329, 11, 6457, 3938, 11, 8895, 257, 2452, 18161, 3209, 281, 360, 264, 912, 13, 51865], "temperature": 0.0, "avg_logprob": -0.2237783974974695, "compression_ratio": 2.127586206896552, "no_speech_prob": 0.00027123047038912773}, {"id": 174, "seek": 76500, "start": 765.0, "end": 766.0, "text": " Google, fairly recently, trained a deep neural network to do the same.", "tokens": [50365, 3329, 11, 6457, 3938, 11, 8895, 257, 2452, 18161, 3209, 281, 360, 264, 912, 13, 50415], "temperature": 0.8, "avg_logprob": -0.6340890520074395, "compression_ratio": 2.0273224043715845, "no_speech_prob": 0.00014885528071317822}, {"id": 175, "seek": 76500, "start": 766.0, "end": 767.0, "text": " And Google, fairly recently, trained a deep neural network to do the same.", "tokens": [50415, 400, 3329, 11, 6457, 3938, 11, 8895, 257, 2452, 18161, 3209, 281, 360, 264, 912, 13, 50465], "temperature": 0.8, "avg_logprob": -0.6340890520074395, "compression_ratio": 2.0273224043715845, "no_speech_prob": 0.00014885528071317822}, {"id": 176, "seek": 76500, "start": 767.0, "end": 772.0, "text": " So for example, in this case it was a news ad that was a little bit less than the average speech recognition time of the day, and even with your big neural network on a large amount of speech, 5,800 hours, that was still much less than they trained their Gaussian mixturing model on.", "tokens": [50465, 407, 337, 1365, 11, 294, 341, 1389, 309, 390, 257, 2583, 614, 300, 390, 257, 707, 857, 1570, 813, 264, 4274, 6218, 11150, 565, 295, 264, 786, 11, 293, 754, 365, 428, 955, 18161, 3209, 322, 257, 2416, 2372, 295, 6218, 11, 1025, 11, 14423, 2496, 11, 300, 390, 920, 709, 1570, 813, 436, 8895, 641, 39148, 2752, 734, 1345, 2316, 322, 13, 50715], "temperature": 0.8, "avg_logprob": -0.6340890520074395, "compression_ratio": 2.0273224043715845, "no_speech_prob": 0.00014885528071317822}, {"id": 177, "seek": 76500, "start": 772.0, "end": 778.0, "text": " But even with much less data, it did a lot better than the technology they had before.", "tokens": [50715, 583, 754, 365, 709, 1570, 1412, 11, 309, 630, 257, 688, 1101, 813, 264, 2899, 436, 632, 949, 13, 51015], "temperature": 0.8, "avg_logprob": -0.6340890520074395, "compression_ratio": 2.0273224043715845, "no_speech_prob": 0.00014885528071317822}, {"id": 178, "seek": 76500, "start": 778.0, "end": 784.0, "text": " So it reduced the error rate from 16% to 12.3%, and the error rate is still falling.", "tokens": [51015, 407, 309, 9212, 264, 6713, 3314, 490, 3165, 4, 281, 2272, 13, 18, 8923, 293, 264, 6713, 3314, 307, 920, 7440, 13, 51315], "temperature": 0.8, "avg_logprob": -0.6340890520074395, "compression_ratio": 2.0273224043715845, "no_speech_prob": 0.00014885528071317822}, {"id": 179, "seek": 76500, "start": 784.0, "end": 793.9, "text": " And in the latest Android, if you do voice search, it's using one of these deep neural networks in order to do very good speech recognition.", "tokens": [51315, 400, 294, 264, 6792, 8853, 11, 498, 291, 360, 3177, 3164, 11, 309, 311, 1228, 472, 295, 613, 2452, 18161, 9590, 294, 1668, 281, 360, 588, 665, 6218, 11150, 13, 51810], "temperature": 0.8, "avg_logprob": -0.6340890520074395, "compression_ratio": 2.0273224043715845, "no_speech_prob": 0.00014885528071317822}], "language": "en"}