Scene: 1 timestamp: 00:00:00.000 - 00:00:20.533 
Neural Networks for Machine Learning
Lecture 1a
Why do we need machine learning?
Geoffrey Hinton
with
Nitish Srivastava
Kevin Swersky
<utterances>
 <utterance number= 1 start=0:00:00 end= 0:00:04
 Hello. Welcome to the Coursera course on neural
<utterance number= 2 start=0:00:04 end= 0:00:09
 networks for machine learning. Before we get into the details of neural
<utterance number= 3 start=0:00:09 end= 0:00:14
 network learning algorithms, I want to talk a little bit about machine learning,
<utterance number= 4 start=0:00:14 end= 0:00:19
 why we need machine learning, the kinds of things we use it for, and show you some
<utterance number= 5 start=0:00:19 end= 0:00:24
 examples of what it can do. So, the reason we need machine learning is

</utterance>
</scene>
</lecture>
</course>Scene: 2 timestamp: 00:00:20.533 - 00:01:18.400 
•
•
What is Machine Learning?
It is very hard to write programs that solve problems like recognizing a
three-dimensional object from a novel viewpoint in new lighting
conditions in a cluttered scene.
-
We don't know what program to write because we don't know
how its done in our brain.
-Even if we had a good idea about how to do it, the program might
be horrendously complicated.
It is hard to write a program to compute the probability that a credit
card transaction is fraudulent.
-
There may not be any rules that are both simple and reliable. We
need to combine a very large number of weak rules.
Fraud is a moving target. The program needs to keep changing.
<utterances>
 <utterance number= 6 start=0:00:24 end= 0:00:29
 that there's some problems where it's very hard to write the programs.
<utterance number= 7 start=0:00:29 end= 0:00:33
 Recognizing a three-dimensional object, for example, when it's from a novel
<utterance number= 8 start=0:00:33 end= 0:00:38
 viewpoint in new lighting conditions in a cluttered scene is very hard to do.
<utterance number= 9 start=0:00:38 end= 0:00:42
 We don't know what program to write because we don't know how it's done in
<utterance number= 10 start=0:00:42 end= 0:00:45
 our brain. And even if we did know what program to
<utterance number= 11 start=0:00:45 end= 0:00:50
 write, it might be that it was a horrendously complicated program.
<utterance number= 12 start=0:00:50 end= 0:00:55
 Another example is detecting a fraudulent credit card transaction, where there may
<utterance number= 13 start=0:00:55 end= 0:00:59
 not be any nice simple rules that will tell you it's fraudulent.
<utterance number= 14 start=0:00:59 end= 0:01:05
 You really need to combine a very large number of not very reliable rules.
<utterance number= 15 start=0:01:05 end= 0:01:10
 And also, those rules change over time because people change the tricks they use
<utterance number= 16 start=0:01:10 end= 0:01:13
 for fraud. So, we need a complicated program that
<utterance number= 17 start=0:01:13 end= 0:01:17
 combines unreliable rules and that we can change easily.
<utterance number= 18 start=0:01:17 end= 0:01:23
 The machine learning approach is to say instead of writing each program by hand

</utterance>
</scene>
</lecture>
</course>Scene: 3 timestamp: 00:01:18.400 - 00:02:22.200 
•
•
The Machine Learning Approach
Instead of writing a program by hand for each specific task, we collect
lots of examples that specify the correct output for a given input.
A machine learning algorithm then takes these examples and produces
a program that does the job.
- The program produced by the learning algorithm may look very
different from a typical hand-written program. It may contain millions
of numbers.
-
-
If we do it right, the program works for new cases as well as the ones
we trained it on.
If the data changes the program can change too by training on the
new data.
Massive amounts of computation are now cheaper than paying
someone to write a task-specific program.
<utterances>
 <utterance number= 19 start=0:01:23 end= 0:01:28
 for each specific task, for a particular task we'll collect a lot of examples that
<utterance number= 20 start=0:01:29 end= 0:01:32
 specify the correct output for a given input.
<utterance number= 21 start=0:01:32 end= 0:01:37
 A machine learning algorithm then takes these examples and produces a program that
<utterance number= 22 start=0:01:37 end= 0:01:41
 does the job. The program produced by the learning
<utterance number= 23 start=0:01:41 end= 0:01:46
 algorithm may look very different from a typical handwritten program.
<utterance number= 24 start=0:01:46 end= 0:01:49
 For example, it might contain millions of numbers about how you weight different
<utterance number= 25 start=0:01:49 end= 0:01:54
 kinds of evidence. If we do it right, the program should work
<utterance number= 26 start=0:01:54 end= 0:01:57
 for new cases as well as the ones it's trained on.
<utterance number= 27 start=0:01:57 end= 0:01:59
 And if the data changes, it might be that the program is not working for new cases
<utterance number= 28 start=0:01:59 end= 0:02:02
 as well as the ones it's trained on. So, if the data changes, we should be able
<utterance number= 29 start=0:02:02 end= 0:02:07
 to change the program relatively easily by retraining it on the new data.
<utterance number= 30 start=0:02:07 end= 0:02:12
 And now, massive amounts of computation are cheaper than paying someone to write a
<utterance number= 31 start=0:02:12 end= 0:02:17
 program for a specific task. So, we can afford big complicated machine
<utterance number= 32 start=0:02:17 end= 0:02:23
 learning programs to produce these task specific systems for us.

</utterance>
</scene>
</lecture>
</course>Scene: 4 timestamp: 00:02:22.200 - 00:03:27.533 
•
•
Some examples of tasks best solved by learning
Recognizing pattems:
-Objects in real scenes
- Facial identities or facial expressions
- Spoken words
Recognizing anomalies:
-
Unusual sequences of credit card transactions
Unusual patterns of sensor readings in a nuclear power plant
Prediction:
-
-
-
Future stock prices or currency exchange rates
Which movies will a person like?
<utterances>
 <utterance number= 33 start=0:02:23 end= 0:02:27
 Some examples of the things that are best done by using a learning algorithm are
<utterance number= 34 start=0:02:27 end= 0:02:29
 recognizing data.
<utterance number= 35 start=0:02:29 end= 0:02:33
 There's also recognizing patterns. So, for example, objects in real scenes.
<utterance number= 36 start=0:02:33 end= 0:02:37
 Or the identities or expressions of people's faces.
<utterance number= 37 start=0:02:37 end= 0:02:44
 Or spoken words. There's also recognizing anomalies.
<utterance number= 38 start=0:02:44 end= 0:02:49
 So, an unusual sequence of credit card transactions would be an anomaly.
<utterance number= 39 start=0:02:49 end= 0:02:54
 Another example of an anomaly would be an unusual pattern of sensor readings in a
<utterance number= 40 start=0:02:54 end= 0:02:57
 nuclear power plant. And you wouldn't really want to have to
<utterance number= 41 start=0:02:57 end= 0:02:59
 deal with those by doing sequence analysis.
<utterance number= 42 start=0:02:59 end= 0:03:02
 And then there's supervised learning where you look at the ones that blow up
<utterance number= 43 start=0:03:02 end= 0:03:06
 and see what, what caused them to blow up. You'd really like to recognize that
<utterance number= 44 start=0:03:06 end= 0:03:10
 something funny is happening without having any supervision signal.
<utterance number= 45 start=0:03:10 end= 0:03:15
 It's just not behaving in its normal way. And then there's prediction.
<utterance number= 46 start=0:03:15 end= 0:03:20
 So, typically predicting future stock prices or currency exchange rates.
<utterance number= 47 start=0:03:20 end= 0:03:24
 Or predicting which movies a person will like from knowing which other movies they
<utterance number= 48 start=0:03:24 end= 0:03:27
 like and which movies a lot of other people liked.
<utterance number= 49 start=0:03:27 end= 0:03:33
 So, in this course, I'm going to use a standard example for explaining a lot of

</utterance>
</scene>
</lecture>
</course>Scene: 5 timestamp: 00:03:27.533 - 00:04:29.867 
•
A standard example of machine learning
A lot of genetics is done on fruit flies.
They are convenient because they breed fast.
We already know a lot about them.
The MNIST database of hand-written digits is the the machine learning
equivalent of fruit flies.
-
-
They are publicly available and we can learn them quite fast in a
moderate-sized neural net.
We know a huge amount about how well various machine learning
methods do on MNIST.
We will use MNIST as our standard task.
<utterances>
 <utterance number= 50 start=0:03:33 end= 0:03:38
 the machine learning algorithms. This is done in a lot of science.
<utterance number= 51 start=0:03:38 end= 0:03:43
 In genetics, for example, a lot of genetics is done on fruit flies.
<utterance number= 52 start=0:03:43 end= 0:03:47
 And the reason is they're convenient. They breed fast.
<utterance number= 53 start=0:03:47 end= 0:03:51
 And a lot is already known about the genetics of fruit flies.
<utterance number= 54 start=0:03:51 end= 0:03:57
 The MNIST database of handwritten digits is the machine learning algorithm that
<utterance number= 55 start=0:03:57 end= 0:04:00
 is the machine learning equivalent of fruit flies.
<utterance number= 56 start=0:04:00 end= 0:04:05
 It's publicly available. We can get machine learning algorithms to
<utterance number= 57 start=0:04:05 end= 0:04:09
 learn how to recognize these handwritten digits quite quickly.
<utterance number= 58 start=0:04:09 end= 0:04:14
 So, it's easy to try lots of variations. And we know huge amounts about how well
<utterance number= 59 start=0:04:14 end= 0:04:17
 different machine learning methods do on MNIST.
<utterance number= 60 start=0:04:17 end= 0:04:21
 And in particular, the different machine learning methods were implemented by
<utterance number= 61 start=0:04:21 end= 0:04:25
 people who believed in them. So, we can rely on those results.
<utterance number= 62 start=0:04:25 end= 0:04:29
 So, for all those reasons, we're going to use MNIST as our standard task.
<utterance number= 63 start=0:04:29 end= 0:04:34
 Here's an example of some of the digits in MNIST.

</utterance>
</scene>
</lecture>
</course>Scene: 6 timestamp: 00:04:29.867 - 00:05:39.867 
It is very hard to say what makes a 2
0001
011(1112
zze
222323
3444445535
6687777888
888894999
<utterances>
 <utterance number= 64 start=0:04:34 end= 0:04:39
 These are ones that were correctly recognized by neural net the first time it
<utterance number= 65 start=0:04:39 end= 0:04:43
 saw them. But they're ones where the neural net
<utterance number= 66 start=0:04:43 end= 0:04:46
 wasn't very confident. And you can see why.
<utterance number= 67 start=0:04:46 end= 0:04:51
 I've arranged these digits in standard scanline order.
<utterance number= 68 start=0:04:51 end= 0:04:54
 So, zeros, then ones, then twos, and so on.
<utterance number= 69 start=0:04:54 end= 0:04:59
 If you look at a bunch of twos, like the ones in the green rectangle, you can see
<utterance number= 70 start=0:04:59 end= 0:05:04
 that if you knew they were a handwritten digit, you'd probably guess they were
<utterance number= 71 start=0:05:04 end= 0:05:07
 twos. But it's very hard to say what it is that
<utterance number= 72 start=0:05:07 end= 0:05:10
 makes them twos. There's nothing simple that they all have
<utterance number= 73 start=0:05:10 end= 0:05:13
 in common. In particular, if you try and overlay one
<utterance number= 74 start=0:05:13 end= 0:05:18
 on another, you'll see it doesn't fit. And even if you skew it a bit, it's very
<utterance number= 75 start=0:05:18 end= 0:05:23
 hard to make them overlay on each other. So, a template isn't going to do the job.
<utterance number= 76 start=0:05:23 end= 0:05:24
 And in particular, if you try and overlay one on another, you'll see it doesn't fit.
<utterance number= 77 start=0:05:24 end= 0:05:28
 And in particular, a template is going to be very hard to find that'll fit those
<utterance number= 78 start=0:05:28 end= 0:05:33
 twos in the green box and won't also fit the things in the red boxes.
<utterance number= 79 start=0:05:33 end= 0:05:38
 So, that's one thing that makes recognizing handwritten digits a good task for machine
<utterance number= 80 start=0:05:38 end= 0:05:41
 learning. Now, I don't want you to think that's the

</utterance>
</scene>
</lecture>
</course>Scene: 7 timestamp: 00:05:39.867 - 00:06:57.867 
•
Beyond MNIST: The ImageNet task
1000 different object classes in 1.3 million high-resolution training images
from the web.
- Best system in 2010 competition got 47% error for its first choice and
25% error for its top 5 choices.
Jitendra Malik (an eminent neural net sceptic) said that this competition is
a good test of whether deep neural networks work well for object
recognition.
-
A very deep neural net (Krizhevsky et. al. 2012) gets less that 40%
error for its first choice and less than 20% for its top 5 choices
(see lecture 5).
<utterances>
 <utterance number= 81 start=0:05:41 end= 0:05:45
 only thing we can do. It's a relatively simple thing for a machine
<utterance number= 82 start=0:05:45 end= 0:05:49
 learning system to do now. And to motivate the rest of the course, I
<utterance number= 83 start=0:05:49 end= 0:05:53
 want to show you some examples of much more difficult things.
<utterance number= 84 start=0:05:53 end= 0:05:58
 So, we now have neural nets with approaching 100 million parameters in
<utterance number= 85 start=0:05:58 end= 0:06:05
 them that can recognize a thousand different object classes in 1.3 million
<utterance number= 86 start=0:06:05 end= 0:06:09
 high resolution training images got from the web.
<utterance number= 87 start=0:06:09 end= 0:06:15
 So, there was a competition in 2010 and the best system got 47% error rate if you
<utterance number= 88 start=0:06:15 end= 0:06:20
 look at its first choice and 25% error rate if you say it got it right if it was
<utterance number= 89 start=0:06:20 end= 0:06:22
 in its top five choices, which isn't bad for a thousand percent error rate.
<utterance number= 90 start=0:06:22 end= 0:06:25
 Which isn't bad for a thousand different objects.
<utterance number= 91 start=0:06:25 end= 0:06:31
 Jitendra Malik, who's an eminent neural net skeptic and a leading computer vision
<utterance number= 92 start=0:06:31 end= 0:06:36
 researcher has said that this competition is a good test of whether deep neural
<utterance number= 93 start=0:06:36 end= 0:06:43
 networks can work well for object recognition. And a very deep neural network can
<utterance number= 94 start=0:06:43 end= 0:06:47
 now do considerably better than the thing that won the competition.
<utterance number= 95 start=0:06:47 end= 0:06:51
 It can get less than 40% error for its first choice and less than 20% error for
<utterance number= 96 start=0:06:51 end= 0:06:52
 its top five choices.
<utterance number= 97 start=0:06:52 end= 0:06:55
 I'll describe that in much more detail in lecture five.
<utterance number= 98 start=0:06:55 end= 0:06:59
 Here's some examples of the kinds of images you have to recognize.

</utterance>
</scene>
</lecture>
</course>Scene: 8 timestamp: 00:06:57.867 - 00:08:19.533 
Some examples from an earlier version of the net
quail
Snowplow
scabbard
otter
snowplow
earthworm
السقاي
ruffed grouse
drilling platform
Meboat
guillotine
arangutan
partridge
garbage truck
broom
50%
<utterances>
 <utterance number= 99 start=0:06:59 end= 0:07:03
 These are images from the test set that it's never seen before.
<utterance number= 100 start=0:07:03 end= 0:07:08
 And below the examples, I'm showing you what the neural net thought the right
<utterance number= 101 start=0:07:08 end= 0:07:13
 answer was, where the length of the horizontal bar is how confident it was,
<utterance number= 102 start=0:07:13 end= 0:07:18
 and the correct answer is in red. So, if you look in the middle, it correctly
<utterance number= 103 start=0:07:18 end= 0:07:21
 identified that as a snowplow. But you can see that it's unbiased.
<utterance number= 104 start=0:07:21 end= 0:07:24
 But you can see that its other choices were also fairly sensible.
<utterance number= 105 start=0:07:24 end= 0:07:27
 It does look a little bit like a drilling platform.
<utterance number= 106 start=0:07:27 end= 0:07:30
 And if you look at its third choice, a lifeboat, it actually looks very like a
<utterance number= 107 start=0:07:30 end= 0:07:32
 lifeboat. You can see the flag on the front of the
<utterance number= 108 start=0:07:32 end= 0:07:36
 boat and the bridge of the boat and the flag at the back and the high surf in the
<utterance number= 109 start=0:07:36 end= 0:07:39
 background. So it's, its errors tell you a lot about
<utterance number= 110 start=0:07:39 end= 0:07:42
 how it's doing it and they're very plausible errors.
<utterance number= 111 start=0:07:42 end= 0:07:47
 If you look on the left, it gets it wrong, possibly because the beak of the bird is
<utterance number= 112 start=0:07:47 end= 0:07:50
 missing and because the feathers of the bird look very like the flag of the bird.
<utterance number= 113 start=0:07:50 end= 0:07:55
 Look very like the wet fur of an otter. But it gets it in its top five and it
<utterance number= 114 start=0:07:55 end= 0:07:58
 does better than me. I wouldn't know if that was a quail or
<utterance number= 115 start=0:07:58 end= 0:08:02
 a roughed grouse or a partridge. If you look on the right, it gets it
<utterance number= 116 start=0:08:02 end= 0:08:06
 completely wrong. It a guillotine, you can see why it says
<utterance number= 117 start=0:08:06 end= 0:08:09
 that. You can possibly see why it says orangutan
<utterance number= 118 start=0:08:09 end= 0:08:12
 because of the sort of jungle looking background and something orange in the
<utterance number= 119 start=0:08:12 end= 0:08:15
 middle. But it fails to get the right answer.
<utterance number= 120 start=0:08:15 end= 0:08:19
 It can, however, deal with a wide range of different objects.
<utterance number= 121 start=0:08:20 end= 0:08:25
 If you look on the left, I would have said microwave was my first answer.

</utterance>
</scene>
</lecture>
</course>Scene: 9 timestamp: 00:08:19.533 - 00:08:44.333 
Mo
shwasher
elect range
shb
It can deal with a wide range of objects
electric range
50%
turnstile
pnson
dining table
badge
burnstile
bulletproof vest
bulletproof vest
polo shirt
50%
knee pad
holster
30%
<utterances>
 <utterance number= 121 start=0:08:20 end= 0:08:25
 If you look on the left, I would have said microwave was my first answer.
<utterance number= 122 start=0:08:25 end= 0:08:29
 The labels aren't very systematic, so actually the correct answer there is
<utterance number= 123 start=0:08:29 end= 0:08:33
 electric range and does get it in its top five.
<utterance number= 124 start=0:08:33 end= 0:08:36
 In the middle, it's getting a turnstile, which is a distributed object.
<utterance number= 125 start=0:08:36 end= 0:08:40
 It does, it can do more than just recognize compact things.
<utterance number= 126 start=0:08:40 end= 0:08:45
 And it can also deal with pictures as well as real scenes, like the bulletproof vest.

</utterance>
</scene>
</lecture>
</course>Scene: 10 timestamp: 00:08:44.333 - 00:09:18.800 
corkscrew
lipstick
screw
ant
It makes some really cool errors
earphone
barber chair
drum
cello
armchair
50%
barber chair
50%
pitcher
I CHITI
measuring cup
coffeepot
ashcan
50%
<utterances>
 <utterance number= 127 start=0:08:45 end= 0:08:50
 And it makes some very cool errors. If you look at the image on the left, it's
<utterance number= 128 start=0:08:50 end= 0:08:54
 an earphone. It doesn't get anything like an earphone.
<utterance number= 129 start=0:08:54 end= 0:08:57
 But if you look at its fourth bet, it thinks it's an ant.
<utterance number= 130 start=0:08:57 end= 0:09:01
 And to begin with, you think that's crazy. But then if you look at it carefully, you
<utterance number= 131 start=0:09:01 end= 0:09:04
 can see it's a view from an ant from underneath. The eyes are looking down at
<utterance number= 132 start=0:09:04 end= 0:09:08
 you, and you can see the antennae behind it. It's not the kind of view of an ant
<utterance number= 133 start=0:09:08 end= 0:09:12
 you'd like to have if you were a green fly. If you look at the one on the right,
<utterance number= 134 start=0:09:12 end= 0:09:16
 it doesn't get the right answer. But all of its answers are cylindrical
<utterance number= 135 start=0:09:16 end= 0:09:20
 objects. Another question that I've been asked is,

</utterance>
</scene>
</lecture>
</course>Scene: 11 timestamp: 00:09:18.800 - 00:10:54.467 
•
•
The Speech Recognition Task
A speech recognition system has several stages:
- Pre-processing: Convert the sound wave into a vector of acoustic
coefficients. Extract a new vector about every 10 mille seconds.
- The acoustic model: Use a few adjacent vectors of acoustic coefficients
to place bets on which part of which phoneme is being spoken.
- Decoding: Find the sequence of bets that does the best job of fitting the
acoustic data and also fitting a model of the kinds of things people say.
Deep neural networks pioneered by George Dahl and Abdel-rahman
Mohamed are now replacing the previous machine learning method
for the acoustic model.
<utterances>
 <utterance number= 136 start=0:09:20 end= 0:09:24
 what is the difference between a sound wave and a speech recognition system?
<utterance number= 137 start=0:09:24 end= 0:09:28
 And another task that neural nets are now very good at is speech recognition.
<utterance number= 138 start=0:09:28 end= 0:09:32
 Or at least part of a speech recognition system. So speech recognition systems have
<utterance number= 139 start=0:09:32 end= 0:09:36
 several stages. First they pre-process the sound wave to
<utterance number= 140 start=0:09:36 end= 0:09:41
 get a vector of acoustic coefficients for each 10 milliseconds of sound wave.
<utterance number= 141 start=0:09:41 end= 0:09:45
 And so they get a hundred of those vectors per second.
<utterance number= 142 start=0:09:45 end= 0:09:50
 They then take a few adjacent vectors of acoustic coefficients, and they need to
<utterance number= 143 start=0:09:50 end= 0:09:55
 place bets on which part of which phoneme is being spoken. So they look at this
<utterance number= 144 start=0:09:55 end= 0:09:59
 little window, and they say in the middle of this window, what do I think the
<utterance number= 145 start=0:09:59 end= 0:10:04
 phoneme is, and which part of the phoneme is it? And a good speech recognition
<utterance number= 146 start=0:10:04 end= 0:10:09
 system will have many alternative models for a phoneme. And each model it might
<utterance number= 147 start=0:10:09 end= 0:10:14
 have three different parts. So it might have many thousands of alternative
<utterance number= 148 start=0:10:14 end= 0:10:18
 fragments that it thinks this might be. And you have to place bets on all those
<utterance number= 149 start=0:10:18 end= 0:10:19
 thousands of alternatives.
<utterance number= 150 start=0:10:20 end= 0:10:25
 And then once you place those bets, you have a decoding stage that does the best
<utterance number= 151 start=0:10:25 end= 0:10:32
 job it can of using plausible bets, but piecing them together into a sequence of
<utterance number= 152 start=0:10:32 end= 0:10:37
 bets that corresponds to the kinds of things that people say.
<utterance number= 153 start=0:10:37 end= 0:10:44
 Currently, deep neural networks pioneered by George Dahl and Abdul Rahman,
<utterance number= 154 start=0:10:44 end= 0:10:48
 Mohammed, at the University of Toronto, are doing better than previous machine
<utterance number= 155 start=0:10:48 end= 0:10:50
 learning methods for the economy.
<utterance number= 156 start=0:10:50 end= 0:10:53
 They're now using the acoustic model, and they're now beginning to be used in
<utterance number= 157 start=0:10:53 end= 0:11:04
 practical systems. So Dahl and Mohammed developed a system that uses many layers

</utterance>
</scene>
</lecture>
</course>Scene: 12 timestamp: 00:10:54.467 - 00:11:55.667 
Phone recognition on the TIMIT benchmark
(Mohamed, Dahl, & Hinton, 2012)
183 HMM-state labels
not pre-trained
2000 logistic hidden units
5 more layers of
pre-trained weights
2000 logistic hidden units
2000 logstic hidden units
15 frames of 40 filterbank outputs
+ their temporal derivatives
-
-
-
After standard post-processing
using a bi-phone model, a deep
net with 8 layers gets 20.7% error
rate.
The best previous speaker-
independent result on TIMIT was
24.4% and this required averaging
several models.
Li Deng (at MSR) realised that this
result could change the way
speech recognition was done.
<utterances>
 <utterance number= 158 start=0:11:04 end= 0:11:14
 of binary neurons to take some acoustic frames and make bets about the labels.
<utterance number= 159 start=0:11:14 end= 0:11:18
 They were doing it on a fairly small database, and they only used 183 alternative
<utterance number= 160 start=0:11:18 end= 0:11:19
 labels.
<utterance number= 161 start=0:11:19 end= 0:11:22
 And to get their system to work well, they did some pre-training, which will be
<utterance number= 162 start=0:11:22 end= 0:11:25
 described in the second half of the course.
<utterance number= 163 start=0:11:25 end= 0:11:31
 After standard post-processing, they got 20.7% error rate on a very standard
<utterance number= 164 start=0:11:31 end= 0:11:35
 benchmark, which is kind of like the MNIST for speech.
<utterance number= 165 start=0:11:35 end= 0:11:40
 The best previous result on that benchmark for speaker independent recognition was
<utterance number= 166 start=0:11:40 end= 0:11:47
 24.4%. And a very experienced speech researcher at Microsoft Research realized that that was a big enough
<utterance number= 167 start=0:11:47 end= 0:11:57
 improvement that probably this would change the way speech recognition systems were done, and indeed it has.

</utterance>
</scene>
</lecture>
</course>Scene: 13 timestamp: 00:11:55.667 - 00:13:14.600 
Word error rates from MSR, IBM, & Google
(Hinton et. al. IEEE Signal Processing Magazine, Nov 2012)
The task
Hours of
training data
Deep neural
network
Gaussian
Mixture
Model
GMM with
more data
Switchboard
309
(Microsoft
18.5%
27.4%
18.6%
(2000 hrs)
Research)
English broadcast
50
17.5%
18.8%
news (IBM)
Google voice
5,870
12.3%
search
(and falling)
16.0%
(>>5.870 hrs)
(android 4.1)
<utterances>
 <utterance number= 168 start=0:11:57 end= 0:12:06
 So if you look at recent results from several different leading speech groups, Microsoft showed that this kind of deep
<utterance number= 169 start=0:12:06 end= 0:12:15
 neural network, when used as the acoustic model in a speech system, reduced the error rate from 27.4% to 18.5%.
<utterance number= 170 start=0:12:15 end= 0:12:23
 And relatively, you could view it as reducing the amount of training data you needed from 2,000 hours down to 309 hours to get comparable performance.
<utterance number= 171 start=0:12:23 end= 0:12:41
 IBM, which has the best system for one of the standard speech recognition tasks for large vocabulary speech recognition, showed that even its very highly tuned system that was getting 18.8% can be beaten by one of these deep neural networks.
<utterance number= 172 start=0:12:41 end= 0:12:43
 And Google fairly recently trained a deep neural network to do the same.
<utterance number= 173 start=0:12:43 end= 0:12:49
 Google fairly recently trained a deep neural network on a large amount of speech, 5,800 hours.
<utterance number= 174 start=0:12:49 end= 0:12:53
 That was still much less than they trained their Gaussian mixture model on.
<utterance number= 175 start=0:12:53 end= 0:12:59
 But even with much less data, it did a lot better than the technology they had before.
<utterance number= 176 start=0:12:59 end= 0:13:04
 So it reduced the error rate from 16% to 12.3%, and the error rate is still falling.
<utterance number= 177 start=0:13:04 end= 0:13:12
 And in the latest Android, if you do voice search, it's using one of these deep neural networks in order to do very good speech recognition.
<utterance number= 178 start=0:13:12 end= 0:13:13
 So it's a very good way to do that.
<utterance number= 179 start=0:13:13 end= 0:13:14
 Thank you.

</utterance>
</scene>
</lecture>
</course>