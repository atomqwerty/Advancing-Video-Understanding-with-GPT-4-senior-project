<course>
<course-name>
Neural Networks for Machine Learning
</course-name>
<lecture>
<lecture-name>
Lecture 16.4.mp4
</lecture-name>
<scene number=1 ,start= 00:00:00.000 ,end= 00:00:13.533> 
<text>
Neural Networks for Machine Learning

Lecture 16d
The fog of progress

Geoffrey Hinton
with

Nitish Srivastava
Kevin Swersky

</text>
<utterances>
 <utterance number= 1 start=0:00:00 end= 0:00:05
 In this final video, I was tempted to make some predictions about the future of
<utterance number= 2 start=0:00:05 end= 0:00:10
 research on neural networks. Instead, I'm going to explain to you why it
<utterance number= 3 start=0:00:10 end= 0:00:14
 would be extremely foolish to try and make any long-term predictions.

</utterance>
</scene>
</lecture>
</course><course>
<course-name>
Neural Networks for Machine Learning
</course-name>
<lecture>
<lecture-name>
Lecture 16.4.mp4
</lecture-name>
<scene number=2 ,start= 00:00:13.533 ,end= 00:01:27.133> 
<text>
Why we cannot predict the long-term future

Consider driving at night. The number of photons you receive fram
the tail-lights of the car in front falls off as I Id?
Now suppose there is fog.

= For small distances its stil 1/d?

— But for big distances its exp(-d) because fog absorbs a certain
fraction of the photons per unit distance

So the car in front becomes gompletely invisible at a distance at
which our short-range 1 / d~ model predicts it will be very visible.

</text>
<utterances>
 <utterance number= 4 start=0:00:14 end= 0:00:19
 I'm going to try and explain why we can't predict the long-term future by using an
<utterance number= 5 start=0:00:19 end= 0:00:23
 analogy. Imagine you're driving a car at night, and
<utterance number= 6 start=0:00:23 end= 0:00:27
 you're looking at the tail lights of the car in front.
<utterance number= 7 start=0:00:27 end= 0:00:32
 The number of photons that you receive from the tail lights of the car in front falls
<utterance number= 8 start=0:00:32 end= 0:00:37
 off as 1 over d squared, where d is the distance to the car in front.
<utterance number= 9 start=0:00:37 end= 0:00:42
 That's assuming that the air is clear. And now suppose there's fog.
<utterance number= 10 start=0:00:42 end= 0:00:47
 Over short ranges, the number of photons you get from the tail lights in front of
<utterance number= 11 start=0:00:47 end= 0:00:52
 you still falls off as 1 over d squared. Because over a short range, the fog hardly
<utterance number= 12 start=0:00:52 end= 0:00:57
 absorbs any light. But for large distances, it falls off as e to the minus d.
<utterance number= 13 start=0:00:57 end= 0:01:00
 And that's because fog has an exponential effect.
<utterance number= 14 start=0:01:00 end= 0:01:05
 Fog absorbs a certain fraction of the photons per unit distance.
<utterance number= 15 start=0:01:05 end= 0:01:10
 So for small distances, fog looks very transparent. But for large distances, it
<utterance number= 16 start=0:01:10 end= 0:01:14
 looks very opaque. So the car in front of us becomes
<utterance number= 17 start=0:01:14 end= 0:01:19
 completely invisible at a distance which our short range model, the 1 over d
<utterance number= 18 start=0:01:19 end= 0:01:22
 squared model, predicts it will be very visible.
<utterance number= 19 start=0:01:22 end= 0:01:26
 That causes people to drive into the back of cars in fog.
<utterance number= 20 start=0:01:26 end= 0:01:27
 It kills people.
<utterance number= 21 start=0:01:27 end= 0:01:31
 The development of technology is also typically exponential.

</utterance>
</scene>
</lecture>
</course><course>
<course-name>
Neural Networks for Machine Learning
</course-name>
<lecture>
<lecture-name>
Lecture 16.4.mp4
</lecture-name>
<scene number=3 ,start= 00:01:27.133 ,end= 00:02:24.800> 
<text>
The effect of exponential progress

Over the short term, things + So the long term future of
change siowly and its easy to machine learning and neural
predict progress. nets is a total mystery.

— We can all make quite - But over the next five
good guesses about what years, its highly probable
will be in the iPhone 6. that big, deep neural

But in the longer run our networks will do amazing
perception of the future hits a things.

wall, just like fog.

</text>
<utterances>
 <utterance number= 22 start=0:01:31 end= 0:01:36
 So over the short term, things appear to change fairly slowly and it's easy to
<utterance number= 23 start=0:01:36 end= 0:01:40
 predict progress. All of us, for example, can probably make quite good guesses
<utterance number= 24 start=0:01:40 end= 0:01:45
 about what will be in the iPhone 6. But in the longer run, our perception of the
<utterance number= 25 start=0:01:45 end= 0:01:48
 future hits a wall, just like with fog.
<utterance number= 26 start=0:01:48 end= 0:01:53
 So the long term future of machine learning and neural nets is really a total mystery.
<utterance number= 27 start=0:01:53 end= 0:01:56
 We've no idea what's going to happen in 30 years time.
<utterance number= 28 start=0:01:56 end= 0:01:59
 There's just no way to predict it from what we know now.
<utterance number= 29 start=0:01:59 end= 0:02:02
 Because we're going to get exponential progress.
<utterance number= 30 start=0:02:02 end= 0:02:07
 In the short run, however, in a period of, say, 3 to 10 years, we can predict it
<utterance number= 31 start=0:02:07 end= 0:02:10
 fairly well. And it seems obvious to me that over the
<utterance number= 32 start=0:02:10 end= 0:02:15
 next 5 years or so, big deep neural networks are going to do amazing things.
<utterance number= 33 start=0:02:15 end= 0:02:20
 I'd like to congratulate all of you who stuck it out long enough to get this far.
<utterance number= 34 start=0:02:20 end= 0:02:25
 I hope you've enjoyed the course and good luck with the final test.

</utterance>
</scene>
</lecture>
</course>