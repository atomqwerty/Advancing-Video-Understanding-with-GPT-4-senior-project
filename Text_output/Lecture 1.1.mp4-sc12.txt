Scene: 12 timestamp: 00:10:54.467 - 00:11:55.667 
Phone recognition on the TIMIT benchmark
(Mohamed, Dahl, & Hinton, 2012)
183 HMM-state labels
not pre-trained
2000 logistic hidden units
5 more layers of
pre-trained weights
2000 logistic hidden units
2000 logstic hidden units
15 frames of 40 filterbank outputs
+ their temporal derivatives
-
-
-
After standard post-processing
using a bi-phone model, a deep
net with 8 layers gets 20.7% error
rate.
The best previous speaker-
independent result on TIMIT was
24.4% and this required averaging
several models.
Li Deng (at MSR) realised that this
result could change the way
speech recognition was done.
<utterances>
 <utterance number= 158 start=0:11:04 end= 0:11:14
 of binary neurons to take some acoustic frames and make bets about the labels.
<utterance number= 159 start=0:11:14 end= 0:11:18
 They were doing it on a fairly small database, and they only used 183 alternative
<utterance number= 160 start=0:11:18 end= 0:11:19
 labels.
<utterance number= 161 start=0:11:19 end= 0:11:22
 And to get their system to work well, they did some pre-training, which will be
<utterance number= 162 start=0:11:22 end= 0:11:25
 described in the second half of the course.
<utterance number= 163 start=0:11:25 end= 0:11:31
 After standard post-processing, they got 20.7% error rate on a very standard
<utterance number= 164 start=0:11:31 end= 0:11:35
 benchmark, which is kind of like the MNIST for speech.
<utterance number= 165 start=0:11:35 end= 0:11:40
 The best previous result on that benchmark for speaker independent recognition was
<utterance number= 166 start=0:11:40 end= 0:11:47
 24.4%. And a very experienced speech researcher at Microsoft Research realized that that was a big enough
<utterance number= 167 start=0:11:47 end= 0:11:57
 improvement that probably this would change the way speech recognition systems were done, and indeed it has.

</utterance>
</scene>
</lecture>
</course>